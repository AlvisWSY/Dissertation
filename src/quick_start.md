# æ¨¡å‹è®­ç»ƒå®éªŒå¿«é€Ÿå…¥é—¨æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æœ¬å®éªŒæ¡†æ¶æä¾›äº†ä¸€ä¸ªå®Œæ•´çš„æ¬ºè¯ˆæ£€æµ‹æ¨¡å‹å¯¹æ¯”ç³»ç»Ÿï¼ŒåŒ…å«ï¼š
- 7ä¸ªä¸åŒçš„æ•°æ®é›†
- 12ç§ä¸åŒç±»åˆ«çš„æœºå™¨å­¦ä¹ ç®—æ³•
- å®Œæ•´çš„è®­ç»ƒã€è¯„ä¼°å’Œå¯è§†åŒ–æµç¨‹

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

ç¡®ä¿å®‰è£…äº†å¿…è¦çš„ä¾èµ–åŒ…ï¼š

```bash
pip install pandas numpy scikit-learn xgboost lightgbm torch matplotlib seaborn
```

### 2. è¿è¡Œå®Œæ•´å®éªŒ

æ‰“å¼€ `experiment.ipynb`ï¼ŒæŒ‰é¡ºåºè¿è¡Œæ‰€æœ‰å•å…ƒæ ¼ï¼š

1. **ç¬¬1èŠ‚**: å¯¼å…¥åº“å’Œé…ç½®
2. **ç¬¬2èŠ‚**: æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
3. **ç¬¬3èŠ‚**: æ¨¡å‹å®šä¹‰
4. **ç¬¬4èŠ‚**: è®­ç»ƒå’Œè¯„ä¼°æ¡†æ¶
5. **ç¬¬5èŠ‚**: ç»“æœå¯è§†åŒ–
6. **ç¬¬6èŠ‚**: è¿è¡Œå®Œæ•´å®éªŒï¼ˆè¿™ä¸€æ­¥ä¼šèŠ±è´¹è¾ƒé•¿æ—¶é—´ï¼‰
7. **ç¬¬7èŠ‚**: æŸ¥çœ‹ç»“æœåˆ†æ
8. **ç¬¬8èŠ‚**: æ·±å…¥åˆ†æå•ä¸ªæ•°æ®é›†ï¼ˆå¯é€‰ï¼‰
9. **ç¬¬9èŠ‚**: æŸ¥çœ‹æ€»ç»“å’Œå»ºè®®

### 3. å¿«é€Ÿæµ‹è¯•å•ä¸ªæ•°æ®é›†

å¦‚æœæƒ³å¿«é€Ÿæµ‹è¯•ï¼Œå¯ä»¥åªè¿è¡Œå•ä¸ªæ•°æ®é›†ï¼š

```python
# é€‰æ‹©ä¸€ä¸ªè¾ƒå°çš„æ•°æ®é›†æµ‹è¯•
from pathlib import Path
import sys
sys.path.append(str(Path.cwd()))

# åˆ›å»ºå®éªŒè¿è¡Œå™¨
runner = ExperimentRunner()

# åŠ è½½æ•°æ®
loader = DatasetLoader('counterfeit_products')  # å°æ•°æ®é›†ï¼Œè®­ç»ƒå¿«
train_df, test_df = loader.load_data()
X_train, X_test, y_train, y_test, _ = loader.preprocess(train_df, test_df)

# è¿è¡Œæ‰€æœ‰æ¨¡å‹
models = runner.run_all_models(X_train, y_train, X_test, y_test, 
                               'counterfeit_products', skip_slow=False)

# æŸ¥çœ‹ç»“æœ
results_df = runner.evaluator.get_results_df()
print(results_df)
```

## ğŸ“Š æ•°æ®é›†è¯´æ˜

### å¤§å‹æ•°æ®é›†ï¼ˆ10ä¸‡æ ·æœ¬ï¼‰
- **IEEE**: 81ç‰¹å¾ï¼Œé«˜ç»´PCAç‰¹å¾ï¼Œæåº¦ä¸å¹³è¡¡
- **col14_behave**: 15ç‰¹å¾ï¼ŒåŒ…å«ç±»åˆ«ç‰¹å¾
- **col16_raw**: 14ç‰¹å¾ï¼Œç”µå•†äº¤æ˜“æ•°æ®
- **creditCardPCA**: 34ç‰¹å¾ï¼ŒPCAå¤„ç†çš„ä¿¡ç”¨å¡æ•°æ®
- **creditCardTransaction**: 13ç‰¹å¾ï¼Œä¿¡ç”¨å¡äº¤æ˜“æ•°æ®

### å°å‹æ•°æ®é›†
- **counterfeit_products**: 4Kè®­ç»ƒ/1Kæµ‹è¯•ï¼Œ16ç‰¹å¾ï¼Œäº§å“çœŸä¼ªæ£€æµ‹
- **counterfeit_transactions**: 2.4Kè®­ç»ƒ/600æµ‹è¯•ï¼Œ19ç‰¹å¾ï¼Œäº¤æ˜“çœŸä¼ªæ£€æµ‹

## ğŸ¤– æ¨¡å‹è¯´æ˜

### ç›‘ç£å­¦ä¹ æ–¹æ³•
1. **Logistic Regression**: çº¿æ€§baselineï¼Œé€Ÿåº¦å¿«
2. **Random Forest**: é›†æˆå­¦ä¹ ï¼Œå¯è§£é‡Šæ€§å¼º
3. **XGBoost**: å¼ºå¤§çš„æ¢¯åº¦æå‡ï¼Œå¤„ç†ä¸å¹³è¡¡æ•°æ®å¥½
4. **LightGBM**: æ›´å¿«çš„æ¢¯åº¦æå‡å®ç°
5. **MLP**: æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œéœ€è¦GPUåŠ é€Ÿ
6. **KNN**: åŸºäºè·ç¦»çš„æ–¹æ³•ï¼Œé€‚åˆå°æ•°æ®é›†

### é™ç»´+åˆ†ç±»
7. **PCA+SVM**: çº¿æ€§é™ç»´+æ”¯æŒå‘é‡æœº
8. **PCA+LR**: çº¿æ€§é™ç»´+é€»è¾‘å›å½’

### æ— ç›‘ç£/å¼‚å¸¸æ£€æµ‹
9. **Isolation Forest**: å¿«é€Ÿå¼‚å¸¸æ£€æµ‹
10. **One-Class SVM**: å•ç±»åˆ†ç±»ï¼Œåªç”¨æ­£å¸¸æ ·æœ¬è®­ç»ƒ
11. **Autoencoder**: æ·±åº¦å­¦ä¹ å¼‚å¸¸æ£€æµ‹

## ğŸ“ˆ è¯„ä¼°æŒ‡æ ‡

- **Accuracy**: æ•´ä½“å‡†ç¡®ç‡
- **Precision**: æŸ¥å‡†ç‡ï¼ˆé¢„æµ‹ä¸ºæ¬ºè¯ˆä¸­çœŸæ­£æ˜¯æ¬ºè¯ˆçš„æ¯”ä¾‹ï¼‰
- **Recall**: æŸ¥å…¨ç‡ï¼ˆæ‰€æœ‰æ¬ºè¯ˆä¸­è¢«æ£€æµ‹å‡ºæ¥çš„æ¯”ä¾‹ï¼‰
- **F1-Score**: Precisionå’ŒRecallçš„è°ƒå’Œå¹³å‡
- **ROC-AUC**: ROCæ›²çº¿ä¸‹é¢ç§¯
- **PR-AUC**: Precision-Recallæ›²çº¿ä¸‹é¢ç§¯
- **è®­ç»ƒæ—¶é—´**: æ¨¡å‹è®­ç»ƒè€—æ—¶
- **æ¨ç†æ—¶é—´**: æ¨¡å‹é¢„æµ‹è€—æ—¶

## ğŸ’¡ ä½¿ç”¨å»ºè®®

### é€‰æ‹©åˆé€‚çš„æ•°æ®é›†
- **å¿«é€Ÿæµ‹è¯•**: ä½¿ç”¨ `counterfeit_products` æˆ– `counterfeit_transactions`
- **å®Œæ•´è¯„ä¼°**: è¿è¡Œæ‰€æœ‰æ•°æ®é›†
- **ç‰¹å®šåœºæ™¯**: æ ¹æ®ä½ çš„åº”ç”¨åœºæ™¯é€‰æ‹©ç›¸ä¼¼çš„æ•°æ®é›†

### é€‰æ‹©åˆé€‚çš„æ¨¡å‹
- **è¿½æ±‚æ€§èƒ½**: XGBoost, LightGBM, Random Forest
- **è¿½æ±‚é€Ÿåº¦**: Logistic Regression, Isolation Forest
- **æ— æ ‡ç­¾æ•°æ®**: Isolation Forest, One-Class SVM, Autoencoder
- **é«˜ç»´æ•°æ®**: PCA+SVM, PCA+LR, MLP

### å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
æ‰€æœ‰ç›‘ç£å­¦ä¹ æ¨¡å‹éƒ½å·²é…ç½®äº†ç±»åˆ«æƒé‡å¹³è¡¡ï¼š
- `class_weight='balanced'` (sklearnæ¨¡å‹)
- `scale_pos_weight` (XGBoost, LightGBM)

## ğŸ“ è¾“å‡ºæ–‡ä»¶

å®éªŒå®Œæˆåä¼šç”Ÿæˆï¼š
- `results/experiment_results.csv`: æ‰€æœ‰æ¨¡å‹çš„è¯¦ç»†ç»“æœ
- Notebookä¸­çš„å¯è§†åŒ–å›¾è¡¨

## ğŸ”§ è‡ªå®šä¹‰é…ç½®

### ä¿®æ”¹æ¨¡å‹è¶…å‚æ•°

åœ¨ `ExperimentRunner` ç±»ä¸­ä¿®æ”¹å„ä¸ªæ¨¡å‹çš„å‚æ•°ï¼š

```python
def run_xgboost(self, X_train, y_train, X_test, y_test, dataset_name):
    model = xgb.XGBClassifier(
        n_estimators=200,  # å¢åŠ æ ‘çš„æ•°é‡
        max_depth=8,       # å¢åŠ æ·±åº¦
        learning_rate=0.05,# é™ä½å­¦ä¹ ç‡
        # ... å…¶ä»–å‚æ•°
    )
```

### æ·»åŠ æ–°æ¨¡å‹

åœ¨ `ExperimentRunner` ç±»ä¸­æ·»åŠ æ–°çš„æ–¹æ³•ï¼š

```python
def run_your_model(self, X_train, y_train, X_test, y_test, dataset_name):
    print("\nğŸš€ è®­ç»ƒ Your Model...")
    start_time = time.time()
    
    # è®­ç»ƒä½ çš„æ¨¡å‹
    model = YourModel()
    model.fit(X_train, y_train)
    train_time = time.time() - start_time
    
    # é¢„æµ‹
    start_time = time.time()
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    inference_time = time.time() - start_time
    
    # è¯„ä¼°
    result = self.evaluator.evaluate_supervised(
        y_test, y_pred, y_pred_proba, 'Your Model', dataset_name,
        train_time, inference_time
    )
    self.evaluator.print_result(result)
    return model
```

ç„¶ååœ¨ `run_all_models` æ–¹æ³•ä¸­è°ƒç”¨å®ƒã€‚

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **å¤§æ•°æ®é›†**: IEEEã€col14_behaveç­‰æ•°æ®é›†è¾ƒå¤§ï¼Œè®­ç»ƒæ—¶é—´è¾ƒé•¿
2. **GPUåŠ é€Ÿ**: MLPå’ŒAutoencoderä¼šè‡ªåŠ¨ä½¿ç”¨GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰
3. **å†…å­˜å ç”¨**: è¿è¡Œæ‰€æœ‰æ¨¡å‹å¯èƒ½éœ€è¦è¾ƒå¤§å†…å­˜
4. **æ…¢é€Ÿæ¨¡å‹**: KNNã€PCA+SVMã€One-Class SVMåœ¨å¤§æ•°æ®é›†ä¸Šä¼šè¢«è‡ªåŠ¨è·³è¿‡

## ğŸ› å¸¸è§é—®é¢˜

### Q: å¯¼å…¥é”™è¯¯
A: ç¡®ä¿æ‰€æœ‰ä¾èµ–åŒ…éƒ½å·²å®‰è£…ï¼š`pip install -r requirements.txt`

### Q: CUDA/GPUé”™è¯¯
A: MLPå’ŒAutoencoderä¼šè‡ªåŠ¨åˆ‡æ¢åˆ°CPUï¼Œä¸å½±å“å…¶ä»–æ¨¡å‹

### Q: å†…å­˜ä¸è¶³
A: å‡å°‘æ•°æ®é›†æ•°é‡æˆ–ä½¿ç”¨ `skip_slow=True` è·³è¿‡æ…¢é€Ÿæ¨¡å‹

### Q: ç»“æœä¸ç†æƒ³
A: æ£€æŸ¥æ•°æ®é¢„å¤„ç†ã€è°ƒæ•´æ¨¡å‹è¶…å‚æ•°ã€å°è¯•ç‰¹å¾å·¥ç¨‹

## ğŸ“š æ‰©å±•é˜…è¯»

- XGBoostæ–‡æ¡£: https://xgboost.readthedocs.io/
- scikit-learnæ–‡æ¡£: https://scikit-learn.org/
- PyTorchæ–‡æ¡£: https://pytorch.org/docs/

## ğŸ¤ è´¡çŒ®

å¦‚æœä½ æœ‰æ”¹è¿›å»ºè®®æˆ–å‘ç°bugï¼Œæ¬¢è¿æå‡ºï¼
