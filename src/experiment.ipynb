{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b0b686c",
   "metadata": {},
   "source": [
    "# Fraud Detection Model comparison Experiment\n",
    "\n",
    "This experiment aims to compare various machine learning and deep learning methods on different fraud detection datasets.\n",
    "\n",
    "## Experiment Design\n",
    "\n",
    "### Dataset Overview (FULL DATA - No Sampling)\n",
    "- **IEEE**: 472K train/118K test, 81 features, High-dimensional PCA features\n",
    "- **col14_behave**: 238K train/59K test, 15 features, Contains categorical features\n",
    "- **col16_raw**: 1.47M train/24K test, 14 features, E-commerce transaction data\n",
    "- **counterfeit_products**: 4K train/1K test, 16 features, Product authenticity detection\n",
    "- **counterfeit_transactions**: 2.4K train/600 test, 19 features, Transaction authenticity detection\n",
    "- **creditCardPCA**: 228K train/57K test, 34 features, PCA-processed credit card data\n",
    "- **creditCardTransaction**: 1.30M train/556K test, 13 features, Credit card transaction data\n",
    "\n",
    "### Model Methods\n",
    "**Supervised Learning:**\n",
    "1. **MLP (Multi-Layer Perceptron)** - Deep learning baseline\n",
    "2. **XGBoost** - Ensemble learning, handles non-linear relationships\n",
    "3. **Random Forest** - Ensemble learning, high interpretability\n",
    "4. **Logistic Regression** - Linear baseline\n",
    "5. **LightGBM** - Efficient gradient boosting\n",
    "\n",
    "**Dimensionality Reduction + Classification:**\n",
    "6. **PCA + SVM** - Linear dimensionality reduction + Support Vector Machine\n",
    "7. **PCA + Logistic Regression** - Linear dimensionality reduction + Linear classification\n",
    "\n",
    "**Distance/Similarity Methods:**\n",
    "8. **KNN** - Distance-based classification\n",
    "\n",
    "**Unsupervised/Semi-supervised Methods:**\n",
    "9. **Isolation Forest** - Anomaly detection\n",
    "10. **One-Class SVM** - One-class classification\n",
    "11. **Autoencoder** - Deep learning anomaly detection\n",
    "12. **DBSCAN** - Density-based clustering\n",
    "\n",
    "### Evaluation Metrics\n",
    "- **Accuracy**: Overall accuracy\n",
    "- **Precision**: Precision rate (for fraud class)\n",
    "- **Recall**: Recall rate (for fraud class)\n",
    "- **F1-Score**: Harmonic mean of precision and recall\n",
    "- **ROC-AUC**: Area under ROC curve\n",
    "- **PR-AUC**: Area under Precision-Recall curve\n",
    "- **Training Time**: Model training time\n",
    "- **Inference Time**: Model prediction time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6c875b",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05ba8364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Detected 2 GPUs:\n",
      "   GPU 0: NVIDIA RTX A5000\n",
      "   Memory: 23.7 GB\n",
      "   GPU 1: NVIDIA RTX A5000\n",
      "   Memory: 23.7 GB\n",
      "‚úÖ All libraries imported successfullyÔºÅ\n"
     ]
    }
   ],
   "source": [
    "# # Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import warnings\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# # Data preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "# imbalance handling\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# supervised learning model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Êó†supervised learning model\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# # Deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# # Evaluation metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, average_precision_score, confusion_matrix,\n",
    "    classification_report, roc_curve, precision_recall_curve\n",
    ")\n",
    "\n",
    "# GPUConfiguration\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'  # Use first two GPUs\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# # Settings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# # Path configuration\n",
    "BASE_DIR = Path('/usr1/home/s124mdg53_07/wang/FYP')\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "JSON_DIR = BASE_DIR / 'json'\n",
    "RESULTS_DIR = BASE_DIR / 'results'\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# GPU Information\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ Detected {torch.cuda.device_count()} GPUs:\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"   GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"   Memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Êú™DetectedGPUÔºåWill useCPU\")\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfullyÔºÅ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf035ece",
   "metadata": {},
   "source": [
    "### 1.1 Helper Functions: Memory Management and GPU Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bb6b5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Utility functions defined!\n",
      "üìä Memory Usage: 0.73 GB\n",
      "   GPU 0 - Allocated: 0.00 GB, Reserved: 0.00 GB\n",
      "   GPU 1 - Allocated: 0.00 GB, Reserved: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "def clear_memory():\n",
    "    \"\"\"Clear memory and GPU cache\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            with torch.cuda.device(i):\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get memory usage\"\"\"\n",
    "    import psutil\n",
    "    process = psutil.Process()\n",
    "    mem_info = process.memory_info()\n",
    "    print(f\"üìä Memory Usage: {mem_info.rss / 1024**3:.2f} GB\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "            reserved = torch.cuda.memory_reserved(i) / 1024**3\n",
    "            print(f\"   GPU {i} - Allocated: {allocated:.2f} GB, Reserved: {reserved:.2f} GB\")\n",
    "\n",
    "def smart_sample(X, y, max_samples=100000, strategy='stratified', min_fraud_samples=100):\n",
    "    \"\"\"\n",
    "    Smart sampling with guaranteed fraud samples for sparse datasets\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : DataFrame\n",
    "        Feature data\n",
    "    y : Series\n",
    "        Label data\n",
    "    max_samples : int\n",
    "        Maximum number of samples\n",
    "    strategy : str\n",
    "        Sampling strategy: 'stratified' (maintains class ratio), 'random'\n",
    "    min_fraud_samples : int\n",
    "        Minimum number of fraud samples to guarantee in sample\n",
    "    \"\"\"\n",
    "    if len(X) <= max_samples:\n",
    "        return X, y\n",
    "    \n",
    "    print(f\"   üìâ Large dataset ({len(X):,} samples), sampling to {max_samples:,} samples\")\n",
    "    \n",
    "    # Check fraud rate\n",
    "    fraud_count = (y == 1).sum()\n",
    "    fraud_rate = fraud_count / len(y)\n",
    "    print(f\"   Original fraud rate: {fraud_rate*100:.4f}% ({fraud_count:,} fraud samples)\")\n",
    "    \n",
    "    if strategy == 'stratified':\n",
    "        # For extremely imbalanced datasets, ensure minimum fraud samples\n",
    "        if fraud_count > 0 and fraud_count < min_fraud_samples:\n",
    "            print(f\"   ‚ö†Ô∏è  Very low fraud samples, ensuring at least {min_fraud_samples} fraud samples in sample\")\n",
    "            # Sample all fraud cases\n",
    "            fraud_idx = y[y == 1].index\n",
    "            normal_idx = y[y == 0].index\n",
    "            \n",
    "            # Sample normal cases\n",
    "            n_normal = min(max_samples - len(fraud_idx), len(normal_idx))\n",
    "            normal_sample_idx = np.random.choice(normal_idx, n_normal, replace=False)\n",
    "            \n",
    "            # Combine indices\n",
    "            sample_idx = np.concatenate([fraud_idx, normal_sample_idx])\n",
    "            np.random.shuffle(sample_idx)\n",
    "            \n",
    "            X_sample = X.loc[sample_idx]\n",
    "            y_sample = y.loc[sample_idx]\n",
    "        else:\n",
    "            # Standard stratified sampling\n",
    "            from sklearn.model_selection import train_test_split\n",
    "            try:\n",
    "                X_sample, _, y_sample, _ = train_test_split(\n",
    "                    X, y, train_size=max_samples, stratify=y, random_state=42\n",
    "                )\n",
    "            except ValueError:\n",
    "                # Fallback to random sampling if stratification fails\n",
    "                print(f\"   ‚ö†Ô∏è  Stratified sampling failed, using random sampling\")\n",
    "                indices = np.random.choice(len(X), max_samples, replace=False)\n",
    "                X_sample = X.iloc[indices]\n",
    "                y_sample = y.iloc[indices]\n",
    "    else:\n",
    "        # Random sampling\n",
    "        indices = np.random.choice(len(X), max_samples, replace=False)\n",
    "        X_sample = X.iloc[indices]\n",
    "        y_sample = y.iloc[indices]\n",
    "    \n",
    "    # Verify fraud samples in result\n",
    "    sampled_fraud = (y_sample == 1).sum()\n",
    "    sampled_fraud_rate = sampled_fraud / len(y_sample)\n",
    "    print(f\"   Sampled fraud rate: {sampled_fraud_rate*100:.4f}% ({sampled_fraud:,} fraud samples)\")\n",
    "    \n",
    "    return X_sample, y_sample\n",
    "\n",
    "print(\"‚úÖ Utility functions defined!\")\n",
    "get_memory_usage()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79c952b",
   "metadata": {},
   "source": [
    "### 1.2 Imbalance Handling Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f9afab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imbalance handler definition completed!\n",
      "   Available strategies: ['none', 'smote', 'adasyn', 'smote_tomek', 'undersample']\n"
     ]
    }
   ],
   "source": [
    "class ImbalanceHandler:\n",
    "    \"\"\"imbalance handlingÂô®\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_imbalance_ratio(y):\n",
    "        \"\"\"Calculate imbalance ratio\"\"\"\n",
    "        counts = y.value_counts()\n",
    "        if len(counts) < 2:\n",
    "            return 1.0\n",
    "        return counts.max() / counts.min()\n",
    "    \n",
    "    @staticmethod\n",
    "    def apply_smote(X, y, sampling_strategy='auto', k_neighbors=5):\n",
    "        \"\"\"SMOTEOversampling\"\"\"\n",
    "        try:\n",
    "            smote = SMOTE(sampling_strategy=sampling_strategy, k_neighbors=k_neighbors, random_state=42)\n",
    "            X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "            return pd.DataFrame(X_resampled, columns=X.columns), pd.Series(y_resampled)\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  SMOTEfailed: {e}, using original data\")\n",
    "            return X, y\n",
    "    \n",
    "    @staticmethod\n",
    "    def apply_adasyn(X, y, sampling_strategy='auto'):\n",
    "        \"\"\"ADASYNAdaptive Oversampling\"\"\"\n",
    "        try:\n",
    "            adasyn = ADASYN(sampling_strategy=sampling_strategy, random_state=42)\n",
    "            X_resampled, y_resampled = adasyn.fit_resample(X, y)\n",
    "            return pd.DataFrame(X_resampled, columns=X.columns), pd.Series(y_resampled)\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  ADASYNfailed: {e}, using original data\")\n",
    "            return X, y\n",
    "    \n",
    "    @staticmethod\n",
    "    def apply_smote_tomek(X, y):\n",
    "        \"\"\"SMOTE + Tomek Links Combined sampling\"\"\"\n",
    "        try:\n",
    "            smt = SMOTETomek(random_state=42)\n",
    "            X_resampled, y_resampled = smt.fit_resample(X, y)\n",
    "            return pd.DataFrame(X_resampled, columns=X.columns), pd.Series(y_resampled)\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  SMOTE-Tomekfailed: {e}, using original data\")\n",
    "            return X, y\n",
    "    \n",
    "    @staticmethod\n",
    "    def apply_undersampling(X, y, sampling_strategy='auto'):\n",
    "        \"\"\"Random undersampling\"\"\"\n",
    "        try:\n",
    "            rus = RandomUnderSampler(sampling_strategy=sampling_strategy, random_state=42)\n",
    "            X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "            return pd.DataFrame(X_resampled, columns=X.columns), pd.Series(y_resampled)\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  Undersamplingfailed: {e}, using original data\")\n",
    "            return X, y\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_class_weights(y):\n",
    "        \"\"\"Calculate class weights\"\"\"\n",
    "        classes = np.unique(y)\n",
    "        weights = compute_class_weight('balanced', classes=classes, y=y)\n",
    "        return dict(zip(classes, weights))\n",
    "\n",
    "\n",
    "# Define imbalance strategies\n",
    "IMBALANCE_STRATEGIES = {\n",
    "    'none': {'name': 'None', 'handler': None},\n",
    "    'smote': {'name': 'SMOTE', 'handler': ImbalanceHandler.apply_smote},\n",
    "    'adasyn': {'name': 'ADASYN', 'handler': ImbalanceHandler.apply_adasyn},\n",
    "    'smote_tomek': {'name': 'SMOTE+Tomek', 'handler': ImbalanceHandler.apply_smote_tomek},\n",
    "    'undersample': {'name': 'Undersampling', 'handler': ImbalanceHandler.apply_undersampling},\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Imbalance handler definition completed!\")\n",
    "print(f\"   Available strategies: {list(IMBALANCE_STRATEGIES.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078e42ff",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "520a87fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing dataset loader with FULL DATA...\n",
      "üìä creditCardPCA - Train: (227845, 34), Test: (56962, 34)\n",
      "\n",
      "Feature Analysis:\n",
      "  - Numerical: 31 features\n",
      "  - Categorical: 1 features\n",
      "  - ID: 0 features (will be removed)\n",
      "  - Timestamp: 1 features (will be removed)\n",
      "\n",
      "Class Imbalance Ratio: 577.29:1\n",
      "Fraud Rate: 0.1729% (394 fraud samples)\n",
      "\n",
      "Encoding categorical features: ['day_of_week']\n",
      "üìä creditCardPCA - Train: (227845, 34), Test: (56962, 34)\n",
      "\n",
      "Feature Analysis:\n",
      "  - Numerical: 31 features\n",
      "  - Categorical: 1 features\n",
      "  - ID: 0 features (will be removed)\n",
      "  - Timestamp: 1 features (will be removed)\n",
      "\n",
      "Class Imbalance Ratio: 577.29:1\n",
      "Fraud Rate: 0.1729% (394 fraud samples)\n",
      "\n",
      "Encoding categorical features: ['day_of_week']\n",
      "   Sparsity: 1.71%\n",
      "\n",
      "‚úÖ Preprocessing complete! Final features: 32\n",
      "   Training set: 227,845 samples\n",
      "   Test set: 56,962 samples\n",
      "   Label distribution - Train: Normal=0.9983, Fraud=0.0017\n",
      "   Label distribution - Test: Normal=0.9983, Fraud=0.0017\n",
      "\n",
      "Sample data shape: X_train=(227845, 32), y_train=(227845,)\n",
      "üìä Memory Usage: 1.02 GB\n",
      "   GPU 0 - Allocated: 0.00 GB, Reserved: 0.00 GB\n",
      "   GPU 1 - Allocated: 0.00 GB, Reserved: 0.00 GB\n",
      "   Sparsity: 1.71%\n",
      "\n",
      "‚úÖ Preprocessing complete! Final features: 32\n",
      "   Training set: 227,845 samples\n",
      "   Test set: 56,962 samples\n",
      "   Label distribution - Train: Normal=0.9983, Fraud=0.0017\n",
      "   Label distribution - Test: Normal=0.9983, Fraud=0.0017\n",
      "\n",
      "Sample data shape: X_train=(227845, 32), y_train=(227845,)\n",
      "üìä Memory Usage: 1.02 GB\n",
      "   GPU 0 - Allocated: 0.00 GB, Reserved: 0.00 GB\n",
      "   GPU 1 - Allocated: 0.00 GB, Reserved: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "class DatasetLoader:\n",
    "    \"\"\"Dataset loader and preprocessor (with sparse data and large dataset optimization)\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_name, data_dir=DATA_DIR, handle_sparse=True, max_samples=None):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.label_encoders = {}\n",
    "        self.scaler = StandardScaler()\n",
    "        self.handle_sparse = handle_sparse  # Whether to handle sparse features\n",
    "        self.max_samples = max_samples  # Maximum samples (for large dataset sampling)\n",
    "        self.is_sparse = False  # Whether it's a sparse dataset\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"Load training and test sets\"\"\"\n",
    "        train_path = self.data_dir / self.dataset_name / 'train'\n",
    "        test_path = self.data_dir / self.dataset_name / 'test'\n",
    "        \n",
    "        # Find CSV files\n",
    "        train_files = list(train_path.glob('*.csv'))\n",
    "        test_files = list(test_path.glob('*.csv'))\n",
    "        \n",
    "        if not train_files or not test_files:\n",
    "            raise FileNotFoundError(f\"Dataset not found: {self.dataset_name}\")\n",
    "        \n",
    "        train_df = pd.read_csv(train_files[0])\n",
    "        test_df = pd.read_csv(test_files[0])\n",
    "        \n",
    "        print(f\"üìä {self.dataset_name} - Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "        \n",
    "        return train_df, test_df\n",
    "    \n",
    "    def identify_label_column(self, df):\n",
    "        \"\"\"Identify label column\"\"\"\n",
    "        label_candidates = ['is_fraud', 'isFraud', 'fraud', 'label', 'target', 'Class']\n",
    "        for col in label_candidates:\n",
    "            if col in df.columns:\n",
    "                return col\n",
    "        raise ValueError(f\"Cannot identify label column for dataset {self.dataset_name}\")\n",
    "    \n",
    "    def identify_feature_types(self, df, label_col):\n",
    "        \"\"\"Identify feature types: numerical, categorical, ID\"\"\"\n",
    "        features = [col for col in df.columns if col != label_col]\n",
    "        \n",
    "        numerical_features = []\n",
    "        categorical_features = []\n",
    "        id_features = []\n",
    "        timestamp_features = []\n",
    "        \n",
    "        for col in features:\n",
    "            # Timestamp features\n",
    "            if 'timestamp' in col.lower() or 'time' in col.lower() or 'date' in col.lower():\n",
    "                timestamp_features.append(col)\n",
    "            # ID features\n",
    "            elif '_id' in col.lower() or col.lower().endswith('id'):\n",
    "                id_features.append(col)\n",
    "            # Categorical features\n",
    "            elif df[col].dtype == 'object' or df[col].nunique() < 20:\n",
    "                categorical_features.append(col)\n",
    "            # Numerical features\n",
    "            else:\n",
    "                numerical_features.append(col)\n",
    "        \n",
    "        return {\n",
    "            'numerical': numerical_features,\n",
    "            'categorical': categorical_features,\n",
    "            'id': id_features,\n",
    "            'timestamp': timestamp_features\n",
    "        }\n",
    "    \n",
    "    def handle_sparse_features(self, X_train, X_test):\n",
    "        \"\"\"\n",
    "        Handle sparse features (for highly sparse datasets like IEEE)\n",
    "        NOTE: Fit on training set only to avoid data leakage\n",
    "        \"\"\"\n",
    "        if not self.handle_sparse:\n",
    "            return X_train, X_test\n",
    "        \n",
    "        # Check sparsity on training set only\n",
    "        sparsity = (X_train == 0).sum().sum() / (X_train.shape[0] * X_train.shape[1])\n",
    "        print(f\"   Sparsity: {sparsity*100:.2f}%\")\n",
    "        \n",
    "        if sparsity > 0.5:  # If sparsity > 50%\n",
    "            self.is_sparse = True\n",
    "            print(f\"   ‚ö†Ô∏è  High sparsity detected, applying sparse feature processing\")\n",
    "            \n",
    "            # Remove all-zero columns (fit on train, apply to both)\n",
    "            zero_cols = X_train.columns[(X_train == 0).all()]\n",
    "            if len(zero_cols) > 0:\n",
    "                print(f\"   Removing {len(zero_cols)} all-zero columns\")\n",
    "                X_train = X_train.drop(columns=zero_cols)\n",
    "                X_test = X_test.drop(columns=zero_cols)\n",
    "            \n",
    "            # Remove low-variance columns (fit on train only)\n",
    "            from sklearn.feature_selection import VarianceThreshold\n",
    "            selector = VarianceThreshold(threshold=0.01)\n",
    "            selector.fit(X_train)  # Fit on training set only\n",
    "            \n",
    "            selected_cols = X_train.columns[selector.get_support()]\n",
    "            removed_cols = len(X_train.columns) - len(selected_cols)\n",
    "            if removed_cols > 0:\n",
    "                print(f\"   Removing {removed_cols} low-variance columns\")\n",
    "                X_train = X_train[selected_cols]\n",
    "                X_test = X_test[selected_cols]\n",
    "        \n",
    "        return X_train, X_test\n",
    "    \n",
    "    def preprocess(self, train_df, test_df, encode_categorical=True, apply_sampling=True):\n",
    "        \"\"\"Preprocess data with no data leakage\"\"\"\n",
    "        # Identify label column\n",
    "        label_col = self.identify_label_column(train_df)\n",
    "        \n",
    "        # Identify feature types (from training set only)\n",
    "        feature_types = self.identify_feature_types(train_df, label_col)\n",
    "        \n",
    "        print(f\"\\nFeature Analysis:\")\n",
    "        print(f\"  - Numerical: {len(feature_types['numerical'])} features\")\n",
    "        print(f\"  - Categorical: {len(feature_types['categorical'])} features\")\n",
    "        print(f\"  - ID: {len(feature_types['id'])} features (will be removed)\")\n",
    "        print(f\"  - Timestamp: {len(feature_types['timestamp'])} features (will be removed)\")\n",
    "        \n",
    "        # Separate features and labels\n",
    "        X_train = train_df.drop(columns=[label_col])\n",
    "        y_train = train_df[label_col]\n",
    "        X_test = test_df.drop(columns=[label_col])\n",
    "        y_test = test_df[label_col]\n",
    "        \n",
    "        # Class imbalance analysis\n",
    "        imbalance_ratio = ImbalanceHandler.get_imbalance_ratio(y_train)\n",
    "        fraud_rate = (y_train == 1).sum() / len(y_train)\n",
    "        print(f\"\\nClass Imbalance Ratio: {imbalance_ratio:.2f}:1\")\n",
    "        print(f\"Fraud Rate: {fraud_rate*100:.4f}% ({(y_train == 1).sum():,} fraud samples)\")\n",
    "        \n",
    "        # Remove ID and timestamp features\n",
    "        drop_cols = feature_types['id'] + feature_types['timestamp']\n",
    "        X_train = X_train.drop(columns=drop_cols, errors='ignore')\n",
    "        X_test = X_test.drop(columns=drop_cols, errors='ignore')\n",
    "        \n",
    "        # Encode categorical features (fit on train, apply to both)\n",
    "        if encode_categorical and feature_types['categorical']:\n",
    "            print(f\"\\nEncoding categorical features: {feature_types['categorical']}\")\n",
    "            for col in feature_types['categorical']:\n",
    "                if col in X_train.columns:\n",
    "                    le = LabelEncoder()\n",
    "                    # Fit on training set\n",
    "                    le.fit(X_train[col].astype(str))\n",
    "                    X_train[col] = le.transform(X_train[col].astype(str))\n",
    "                    # Handle unseen categories in test set\n",
    "                    X_test[col] = X_test[col].astype(str).apply(\n",
    "                        lambda x: le.transform([x])[0] if x in le.classes_ else -1\n",
    "                    )\n",
    "                    self.label_encoders[col] = le\n",
    "        \n",
    "        # Handle sparse features (NO DATA LEAKAGE)\n",
    "        X_train, X_test = self.handle_sparse_features(X_train, X_test)\n",
    "        \n",
    "        # Smart sampling (on training set only, after feature selection)\n",
    "        if apply_sampling and self.max_samples and len(X_train) > self.max_samples:\n",
    "            X_train, y_train = smart_sample(X_train, y_train, self.max_samples, min_fraud_samples=100)\n",
    "        \n",
    "        # Ensure y_train and y_test are 1D Series (not DataFrame)\n",
    "        if isinstance(y_train, pd.DataFrame):\n",
    "            y_train = y_train.iloc[:, 0]\n",
    "        if isinstance(y_test, pd.DataFrame):\n",
    "            y_test = y_test.iloc[:, 0]\n",
    "        \n",
    "        # Reset index to avoid alignment issues\n",
    "        X_train = X_train.reset_index(drop=True)\n",
    "        X_test = X_test.reset_index(drop=True)\n",
    "        y_train = y_train.reset_index(drop=True)\n",
    "        y_test = y_test.reset_index(drop=True)\n",
    "        \n",
    "        # Standardize numerical features (fit on train, apply to both)\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        \n",
    "        # Convert to DataFrame to keep column names\n",
    "        X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "        X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Preprocessing complete! Final features: {X_train_scaled.shape[1]}\")\n",
    "        print(f\"   Training set: {X_train_scaled.shape[0]:,} samples\")\n",
    "        print(f\"   Test set: {X_test_scaled.shape[0]:,} samples\")\n",
    "        train_fraud_rate = (y_train == 1).sum() / len(y_train)\n",
    "        test_fraud_rate = (y_test == 1).sum() / len(y_test)\n",
    "        print(f\"   Label distribution - Train: Normal={1-train_fraud_rate:.4f}, Fraud={train_fraud_rate:.4f}\")\n",
    "        print(f\"   Label distribution - Test: Normal={1-test_fraud_rate:.4f}, Fraud={test_fraud_rate:.4f}\")\n",
    "        \n",
    "        # Clear memory\n",
    "        clear_memory()\n",
    "        \n",
    "        return X_train_scaled, X_test_scaled, y_train, y_test, feature_types\n",
    "\n",
    "# Test dataset loader with FULL DATA (no sampling)\n",
    "print(\"üß™ Testing dataset loader with FULL DATA...\")\n",
    "loader = DatasetLoader('creditCardPCA', max_samples=None)  # No sampling limit\n",
    "train_df, test_df = loader.load_data()\n",
    "X_train, X_test, y_train, y_test, feature_types = loader.preprocess(train_df, test_df)\n",
    "print(f\"\\nSample data shape: X_train={X_train.shape}, y_train={y_train.shape}\")\n",
    "get_memory_usage()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b37f7a9",
   "metadata": {},
   "source": [
    "## 3. Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2a7a5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model DefinitionscompletedÔºÅ\n",
      "   Will use 2 GPUsfor accelerated training\n"
     ]
    }
   ],
   "source": [
    "# ==================== # Deep Learning Models (Multi-GPU Support) ====================\n",
    "\n",
    "class MLPClassifier(nn.Module):\n",
    "    \"\"\"Multi-Layer Perceptron Classifier (Optimized)\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dims=[256, 128, 64], dropout=0.3):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    \"\"\"Autoencoder for Anomaly Detection (Optimized)\"\"\"\n",
    "    def __init__(self, input_dim, encoding_dims=[128, 64, 32]):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for encoding_dim in encoding_dims:\n",
    "            encoder_layers.extend([\n",
    "                nn.Linear(prev_dim, encoding_dim),\n",
    "                nn.BatchNorm1d(encoding_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2)\n",
    "            ])\n",
    "            prev_dim = encoding_dim\n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        \n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        decoding_dims = list(reversed(encoding_dims[:-1])) + [input_dim]\n",
    "        for decoding_dim in decoding_dims:\n",
    "            decoder_layers.extend([\n",
    "                nn.Linear(prev_dim, decoding_dim),\n",
    "                nn.BatchNorm1d(decoding_dim) if decoding_dim != input_dim else nn.Identity(),\n",
    "                nn.ReLU() if decoding_dim != input_dim else nn.Identity()\n",
    "            ])\n",
    "            prev_dim = decoding_dim\n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "    def get_reconstruction_error(self, x):\n",
    "        with torch.no_grad():\n",
    "            reconstructed = self.forward(x)\n",
    "            error = torch.mean((x - reconstructed) ** 2, dim=1)\n",
    "        return error.cpu().numpy()\n",
    "\n",
    "\n",
    "# ==================== Training Functions (Optimized) ====================\n",
    "\n",
    "def train_mlp(model, X_train, y_train, X_val, y_val, epochs=50, batch_size=512, \n",
    "              lr=0.001, patience=10, use_multi_gpu=True):\n",
    "    \"\"\"Train MLP model (Multi-GPU and class weights support)\"\"\"\n",
    "    # GPUConfiguration\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Â§öGPUData parallelism\n",
    "        if use_multi_gpu and torch.cuda.device_count() > 1:\n",
    "            print(f\"   Using {torch.cuda.device_count()} GPUsfor training\")\n",
    "            model = nn.DataParallel(model, device_ids=[0, 1])\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        model = model.to(device)\n",
    "    \n",
    "    # Data preparation\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.FloatTensor(X_train.values),\n",
    "        torch.FloatTensor(y_train.values).unsqueeze(1)\n",
    "    )\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
    "                             num_workers=4, pin_memory=True)\n",
    "    \n",
    "    X_val_tensor = torch.FloatTensor(X_val.values).to(device)\n",
    "    y_val_tensor = torch.FloatTensor(y_val.values).unsqueeze(1).to(device)\n",
    "    \n",
    "    # Calculate class weights\n",
    "    pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "    pos_weight_tensor = torch.tensor([pos_weight], dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)  # Using weighted BCE\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5)\n",
    "    \n",
    "    # Training\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            # Remove final sigmoid, use logits\n",
    "            if isinstance(model, nn.DataParallel):\n",
    "                outputs = model.module.network[:-1](batch_X)  # Remove sigmoid\n",
    "            else:\n",
    "                outputs = model.network[:-1](batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            if isinstance(model, nn.DataParallel):\n",
    "                val_outputs = model.module.network[:-1](X_val_tensor)\n",
    "            else:\n",
    "                val_outputs = model.network[:-1](X_val_tensor)\n",
    "            val_loss = criterion(val_outputs, y_val_tensor).item()\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"   Epoch {epoch+1}/{epochs} - Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"   Early stopping‰∫éepoch {epoch+1}\")\n",
    "                model.load_state_dict(best_model_state)\n",
    "                break\n",
    "    \n",
    "    # Clear GPU cache\n",
    "    clear_memory()\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def train_autoencoder(model, X_train, epochs=50, batch_size=512, lr=0.001, use_multi_gpu=True):\n",
    "    \"\"\"Train Autoencoder (Multi-GPU support)\"\"\"\n",
    "    # GPUConfiguration\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Â§öGPUData parallelism\n",
    "        if use_multi_gpu and torch.cuda.device_count() > 1:\n",
    "            print(f\"   Using {torch.cuda.device_count()} GPUsfor training\")\n",
    "            model = nn.DataParallel(model, device_ids=[0, 1])\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        model = model.to(device)\n",
    "    \n",
    "    # Âè™UsingNormalsamplesTraining\n",
    "    train_dataset = TensorDataset(torch.FloatTensor(X_train.values))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                             num_workers=4, pin_memory=True)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_X, in train_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_X)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"   Epoch {epoch+1}/{epochs} - Train Loss: {train_loss/len(train_loader):.4f}\")\n",
    "    \n",
    "    # Clear GPU cache\n",
    "    clear_memory()\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "print(\"‚úÖ Model DefinitionscompletedÔºÅ\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   Will use {torch.cuda.device_count()} GPUsfor accelerated training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0996af3c",
   "metadata": {},
   "source": [
    "## 4. Unified Model Training and Evaluation Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "657b82cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ËØÑ‰º∞andimbalanceprocessingÊ®°ÂùóÊûÑÂª∫completedÔºÅ\n",
      "‚úÖ ExperimentRunner ÊûÑÂª∫completedÔºàÊîØÊåÅimbalancecomparisonÔºâÔºÅ\n"
     ]
    }
   ],
   "source": [
    "class PerformanceEvaluator:\n",
    "    \"\"\"Performance Evaluator (supports imbalance handling annotation)\"\"\"\n",
    "    \n",
    "    def evaluate_supervised(self, y_true, y_pred, y_pred_proba, model_name, dataset_name, \n",
    "                          train_time, inference_time, imbalance_strategy='none'):\n",
    "        \"\"\"Evaluate supervised learning model\"\"\"\n",
    "        result = {\n",
    "            'model': model_name,\n",
    "            'dataset': dataset_name,\n",
    "            'imbalance_strategy': imbalance_strategy,\n",
    "            'accuracy': accuracy_score(y_true, y_pred),\n",
    "            'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "            'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "            'f1_score': f1_score(y_true, y_pred, zero_division=0),\n",
    "            'roc_auc': roc_auc_score(y_true, y_pred_proba) if len(np.unique(y_true)) > 1 else 0.0,\n",
    "            'train_time': train_time,\n",
    "            'inference_time': inference_time\n",
    "        }\n",
    "        return result\n",
    "    \n",
    "    def evaluate_unsupervised(self, y_true, anomaly_scores, model_name, dataset_name,\n",
    "                             train_time, inference_time, contamination, imbalance_strategy='none'):\n",
    "        \"\"\"Evaluate unsupervised learning model (anomaly detection)\"\"\"\n",
    "        # UsingÊúÄ‰ºòÈòàValue\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, anomaly_scores)\n",
    "        optimal_idx = np.argmax(tpr - fpr)\n",
    "        optimal_threshold = thresholds[optimal_idx]\n",
    "        y_pred = (anomaly_scores > optimal_threshold).astype(int)\n",
    "        \n",
    "        result = {\n",
    "            'model': model_name,\n",
    "            'dataset': dataset_name,\n",
    "            'imbalance_strategy': imbalance_strategy,\n",
    "            'accuracy': accuracy_score(y_true, y_pred),\n",
    "            'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "            'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "            'f1_score': f1_score(y_true, y_pred, zero_division=0),\n",
    "            'roc_auc': roc_auc_score(y_true, anomaly_scores) if len(np.unique(y_true)) > 1 else 0.0,\n",
    "            'train_time': train_time,\n",
    "            'inference_time': inference_time\n",
    "        }\n",
    "        return result\n",
    "    \n",
    "    def print_result(self, result):\n",
    "        \"\"\"Print evaluation results\"\"\"\n",
    "        print(f\"\\n‚úÖ {result['model']} completed!\")\n",
    "        print(f\"   Accuracy: {result['accuracy']:.4f}\")\n",
    "        print(f\"   Precision: {result['precision']:.4f}\")\n",
    "        print(f\"   Recall: {result['recall']:.4f}\")\n",
    "        print(f\"   F1-Score: {result['f1_score']:.4f}\")\n",
    "        print(f\"   ROC-AUC: {result['roc_auc']:.4f}\")\n",
    "        print(f\"   TrainingTime: {result['train_time']:.2f}s\")\n",
    "        print(f\"   Inference time: {result['inference_time']:.2f}s\")\n",
    "\n",
    "\n",
    "class ImbalanceHandler:\n",
    "    \"\"\"imbalance handlingÂô®\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_imbalance_ratio(y):\n",
    "        \"\"\"Calculate class imbalance ratio\"\"\"\n",
    "        class_counts = pd.Series(y).value_counts()\n",
    "        if len(class_counts) < 2:\n",
    "            return 1.0\n",
    "        return class_counts.max() / class_counts.min()\n",
    "    \n",
    "    @staticmethod\n",
    "    def apply_smote(X, y):\n",
    "        \"\"\"Apply SMOTE oversampling\"\"\"\n",
    "        smote = SMOTE(random_state=42)\n",
    "        try:\n",
    "            X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "            return X_resampled, y_resampled\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  SMOTEfailed: {e}Ôºåusing original data\")\n",
    "            return X, y\n",
    "    \n",
    "    @staticmethod\n",
    "    def apply_adasyn(X, y):\n",
    "        \"\"\"Apply ADASYN oversampling\"\"\"\n",
    "        adasyn = ADASYN(random_state=42)\n",
    "        try:\n",
    "            X_resampled, y_resampled = adasyn.fit_resample(X, y)\n",
    "            return X_resampled, y_resampled\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  ADASYNfailed: {e}Ôºåusing original data\")\n",
    "            return X, y\n",
    "    \n",
    "    @staticmethod\n",
    "    def apply_smote_tomek(X, y):\n",
    "        \"\"\"Apply SMOTE+Tomek Links combined sampling\"\"\"\n",
    "        smote_tomek = SMOTETomek(random_state=42)\n",
    "        try:\n",
    "            X_resampled, y_resampled = smote_tomek.fit_resample(X, y)\n",
    "            return X_resampled, y_resampled\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  SMOTE+Tomekfailed: {e}Ôºåusing original data\")\n",
    "            return X, y\n",
    "    \n",
    "    @staticmethod\n",
    "    def apply_undersampling(X, y):\n",
    "        \"\"\"Apply random undersampling\"\"\"\n",
    "        rus = RandomUnderSampler(random_state=42)\n",
    "        try:\n",
    "            X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "            return X_resampled, y_resampled\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Undersamplingfailed: {e}Ôºåusing original data\")\n",
    "            return X, y\n",
    "\n",
    "\n",
    "# Define imbalance handling strategies\n",
    "IMBALANCE_STRATEGIES = {\n",
    "    'none': {'name': 'None', 'func': None},\n",
    "    'smote': {'name': 'SMOTE', 'func': ImbalanceHandler.apply_smote},\n",
    "    'adasyn': {'name': 'ADASYN', 'func': ImbalanceHandler.apply_adasyn},\n",
    "    'smote_tomek': {'name': 'SMOTE+Tomek', 'func': ImbalanceHandler.apply_smote_tomek},\n",
    "    'undersampling': {'name': 'Undersampling', 'func': ImbalanceHandler.apply_undersampling},\n",
    "}\n",
    "\n",
    "\n",
    "print(\"‚úÖ ËØÑ‰º∞andimbalanceprocessingÊ®°ÂùóÊûÑÂª∫completedÔºÅ\")\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## üöÄ Model Training Module (12 Models)\n",
    "\n",
    "# %%\n",
    "class ExperimentRunner:\n",
    "    \"\"\"Experiment Runner (supports imbalance handling comparison)\"\"\"\n",
    "    \n",
    "    def __init__(self, compare_imbalance=True, use_sampling_for_slow_models=True):\n",
    "        self.evaluator = PerformanceEvaluator()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.compare_imbalance = compare_imbalance  # Whether to compare imbalance handling methods\n",
    "        self.use_sampling_for_slow_models = use_sampling_for_slow_models  # Whether to sample for slow models\n",
    "    \n",
    "    def _apply_imbalance_strategy(self, X, y, strategy='none'):\n",
    "        \"\"\"Apply imbalance handling strategy\"\"\"\n",
    "        if strategy == 'none' or IMBALANCE_STRATEGIES[strategy]['func'] is None:\n",
    "            return X, y\n",
    "        \n",
    "        strategy_func = IMBALANCE_STRATEGIES[strategy]['func']\n",
    "        return strategy_func(X, y)\n",
    "    \n",
    "    def run_logistic_regression(self, X_train, y_train, X_test, y_test, dataset_name, imbalance_strategy='none'):\n",
    "        \"\"\"Logistic Regression\"\"\"\n",
    "        print(f\"\\nüöÄ Training Logistic Regression [{IMBALANCE_STRATEGIES[imbalance_strategy]['name']}]...\")\n",
    "        \n",
    "        X_train_proc, y_train_proc = self._apply_imbalance_strategy(X_train, y_train, imbalance_strategy)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        model = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced', n_jobs=-1)\n",
    "        model.fit(X_train_proc, y_train_proc)\n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        start_time = time.time()\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        inference_time = time.time() - start_time\n",
    "        \n",
    "        result = self.evaluator.evaluate_supervised(\n",
    "            y_test, y_pred, y_pred_proba, 'Logistic Regression', dataset_name,\n",
    "            train_time, inference_time, imbalance_strategy\n",
    "        )\n",
    "        self.evaluator.print_result(result)\n",
    "        \n",
    "        del model, X_train_proc, y_train_proc\n",
    "        clear_memory()\n",
    "        return result\n",
    "    \n",
    "    def run_random_forest(self, X_train, y_train, X_test, y_test, dataset_name, imbalance_strategy='none'):\n",
    "        \"\"\"Random Forest\"\"\"\n",
    "        print(f\"\\nüöÄ Training Random Forest [{IMBALANCE_STRATEGIES[imbalance_strategy]['name']}]...\")\n",
    "        \n",
    "        X_train_proc, y_train_proc = self._apply_imbalance_strategy(X_train, y_train, imbalance_strategy)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=100, max_depth=10, random_state=42, \n",
    "            class_weight='balanced', n_jobs=-1\n",
    "        )\n",
    "        model.fit(X_train_proc, y_train_proc)\n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        start_time = time.time()\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        inference_time = time.time() - start_time\n",
    "        \n",
    "        result = self.evaluator.evaluate_supervised(\n",
    "            y_test, y_pred, y_pred_proba, 'Random Forest', dataset_name,\n",
    "            train_time, inference_time, imbalance_strategy\n",
    "        )\n",
    "        self.evaluator.print_result(result)\n",
    "        \n",
    "        del model, X_train_proc, y_train_proc\n",
    "        clear_memory()\n",
    "        return result\n",
    "    \n",
    "    def run_xgboost(self, X_train, y_train, X_test, y_test, dataset_name, imbalance_strategy='none'):\n",
    "        \"\"\"XGBoost\"\"\"\n",
    "        print(f\"\\nüöÄ Training XGBoost [{IMBALANCE_STRATEGIES[imbalance_strategy]['name']}]...\")\n",
    "        \n",
    "        X_train_proc, y_train_proc = self._apply_imbalance_strategy(X_train, y_train, imbalance_strategy)\n",
    "        \n",
    "        scale_pos_weight = (y_train_proc == 0).sum() / (y_train_proc == 1).sum()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        model = xgb.XGBClassifier(\n",
    "            n_estimators=100, max_depth=6, learning_rate=0.1,\n",
    "            scale_pos_weight=scale_pos_weight, random_state=42,\n",
    "            eval_metric='logloss', tree_method='gpu_hist',  # GPUacceleration\n",
    "            gpu_id=0\n",
    "        )\n",
    "        model.fit(X_train_proc, y_train_proc)\n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        start_time = time.time()\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        inference_time = time.time() - start_time\n",
    "        \n",
    "        result = self.evaluator.evaluate_supervised(\n",
    "            y_test, y_pred, y_pred_proba, 'XGBoost', dataset_name,\n",
    "            train_time, inference_time, imbalance_strategy\n",
    "        )\n",
    "        self.evaluator.print_result(result)\n",
    "        \n",
    "        del model, X_train_proc, y_train_proc\n",
    "        clear_memory()\n",
    "        return result\n",
    "    \n",
    "    def run_lightgbm(self, X_train, y_train, X_test, y_test, dataset_name, imbalance_strategy='none'):\n",
    "        \"\"\"LightGBM\"\"\"\n",
    "        print(f\"\\nüöÄ Training LightGBM [{IMBALANCE_STRATEGIES[imbalance_strategy]['name']}]...\")\n",
    "        \n",
    "        X_train_proc, y_train_proc = self._apply_imbalance_strategy(X_train, y_train, imbalance_strategy)\n",
    "        \n",
    "        scale_pos_weight = (y_train_proc == 0).sum() / (y_train_proc == 1).sum()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        model = lgb.LGBMClassifier(\n",
    "            n_estimators=100, max_depth=6, learning_rate=0.1,\n",
    "            scale_pos_weight=scale_pos_weight, random_state=42,\n",
    "            verbose=-1, device='cpu', n_jobs=-1  # ‚úÖ Fixed: Use CPU instead of GPU\n",
    "        )\n",
    "        model.fit(X_train_proc, y_train_proc)\n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        start_time = time.time()\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        inference_time = time.time() - start_time\n",
    "        \n",
    "        result = self.evaluator.evaluate_supervised(\n",
    "            y_test, y_pred, y_pred_proba, 'LightGBM', dataset_name,\n",
    "            train_time, inference_time, imbalance_strategy\n",
    "        )\n",
    "        self.evaluator.print_result(result)\n",
    "        \n",
    "        del model, X_train_proc, y_train_proc\n",
    "        clear_memory()\n",
    "        return result\n",
    "    \n",
    "    def run_knn(self, X_train, y_train, X_test, y_test, dataset_name, imbalance_strategy='none', max_samples=20000):\n",
    "        \"\"\"K-Nearest NeighborsÔºàÂ§ßDatasetÈááÊ†∑Ôºâ\"\"\"\n",
    "        print(f\"\\nüöÄ Training KNN [{IMBALANCE_STRATEGIES[imbalance_strategy]['name']}]...\")\n",
    "        \n",
    "        # forÂ§ßDatasetÈááÊ†∑\n",
    "        if self.use_sampling_for_slow_models and len(X_train) > max_samples:\n",
    "            X_train_sampled, y_train_sampled = smart_sample(X_train, y_train, max_samples)\n",
    "        else:\n",
    "            X_train_sampled, y_train_sampled = X_train, y_train\n",
    "        \n",
    "        X_train_proc, y_train_proc = self._apply_imbalance_strategy(X_train_sampled, y_train_sampled, imbalance_strategy)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        model = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n",
    "        model.fit(X_train_proc, y_train_proc)\n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        start_time = time.time()\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        inference_time = time.time() - start_time\n",
    "        \n",
    "        result = self.evaluator.evaluate_supervised(\n",
    "            y_test, y_pred, y_pred_proba, 'KNN', dataset_name,\n",
    "            train_time, inference_time, imbalance_strategy\n",
    "        )\n",
    "        self.evaluator.print_result(result)\n",
    "        \n",
    "        del model, X_train_proc, y_train_proc\n",
    "        clear_memory()\n",
    "        return result\n",
    "    \n",
    "    def run_pca_svm(self, X_train, y_train, X_test, y_test, dataset_name, \n",
    "                    imbalance_strategy='none', n_components=0.95, max_samples=20000):\n",
    "        \"\"\"PCA + SVMÔºàÂ§ßDatasetÈááÊ†∑Ôºâ\"\"\"\n",
    "        print(f\"\\nüöÄ Training PCA + SVM [{IMBALANCE_STRATEGIES[imbalance_strategy]['name']}]...\")\n",
    "        \n",
    "        # forÂ§ßDatasetÈááÊ†∑\n",
    "        if self.use_sampling_for_slow_models and len(X_train) > max_samples:\n",
    "            X_train_sampled, y_train_sampled = smart_sample(X_train, y_train, max_samples)\n",
    "        else:\n",
    "            X_train_sampled, y_train_sampled = X_train, y_train\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # PCAdimension reduction\n",
    "        pca = PCA(n_components=n_components, random_state=42)\n",
    "        X_train_pca = pca.fit_transform(X_train_sampled)\n",
    "        X_test_pca = pca.transform(X_test)\n",
    "        print(f\"   PCAdimension reduction: {X_train_sampled.shape[1]} -> {X_train_pca.shape[1]} dimensions\")\n",
    "        \n",
    "        # Â∫îwithimbalanceprocessing\n",
    "        X_train_pca_df = pd.DataFrame(X_train_pca)\n",
    "        X_train_proc, y_train_proc = self._apply_imbalance_strategy(X_train_pca_df, y_train_sampled, imbalance_strategy)\n",
    "        \n",
    "        # SVMÂàÜÁ±ª\n",
    "        model = SVC(kernel='rbf', probability=True, random_state=42, class_weight='balanced')\n",
    "        model.fit(X_train_proc, y_train_proc)\n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        start_time = time.time()\n",
    "        y_pred = model.predict(X_test_pca)\n",
    "        y_pred_proba = model.predict_proba(X_test_pca)[:, 1]\n",
    "        inference_time = time.time() - start_time\n",
    "        \n",
    "        result = self.evaluator.evaluate_supervised(\n",
    "            y_test, y_pred, y_pred_proba, 'PCA+SVM', dataset_name,\n",
    "            train_time, inference_time, imbalance_strategy\n",
    "        )\n",
    "        self.evaluator.print_result(result)\n",
    "        \n",
    "        del pca, model, X_train_proc, y_train_proc, X_train_pca, X_test_pca\n",
    "        clear_memory()\n",
    "        return result\n",
    "    \n",
    "    def run_mlp(self, X_train, y_train, X_test, y_test, dataset_name, imbalance_strategy='none'):\n",
    "        \"\"\"MLP (Â§öÂ±ÇÊÑüÁü•Êú∫)\"\"\"\n",
    "        print(f\"\\nüöÄ Training MLP [{IMBALANCE_STRATEGIES[imbalance_strategy]['name']}]...\")\n",
    "        \n",
    "        X_train_proc, y_train_proc = self._apply_imbalance_strategy(X_train, y_train, imbalance_strategy)\n",
    "        \n",
    "        # ÂàíÂàÜValidationÈõÜ\n",
    "        X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "            X_train_proc, y_train_proc, test_size=0.2, random_state=42, stratify=y_train_proc\n",
    "        )\n",
    "        \n",
    "        start_time = time.time()\n",
    "        model = MLPClassifier(input_dim=X_train.shape[1], hidden_dims=[256, 128, 64])\n",
    "        model = train_mlp(model, X_train_split, y_train_split, X_val, y_val, epochs=50, use_multi_gpu=True)\n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        # È¢ÑÊµã\n",
    "        model.eval()\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            X_test_tensor = torch.FloatTensor(X_test.values).to(self.device)\n",
    "            if isinstance(model, nn.DataParallel):\n",
    "                y_pred_proba = model.module(X_test_tensor).cpu().numpy().flatten()\n",
    "            else:\n",
    "                y_pred_proba = model(X_test_tensor).cpu().numpy().flatten()\n",
    "        y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "        inference_time = time.time() - start_time\n",
    "        \n",
    "        result = self.evaluator.evaluate_supervised(\n",
    "            y_test, y_pred, y_pred_proba, 'MLP', dataset_name,\n",
    "            train_time, inference_time, imbalance_strategy\n",
    "        )\n",
    "        self.evaluator.print_result(result)\n",
    "        \n",
    "        del model, X_train_proc, y_train_proc\n",
    "        clear_memory()\n",
    "        return result\n",
    "    \n",
    "    def run_isolation_forest(self, X_train, y_train, X_test, y_test, dataset_name, imbalance_strategy='none'):\n",
    "        \"\"\"Isolation Forest(Unsupervised, no resampling needed)\"\"\"\n",
    "        print(f\"\\nüöÄ Training Isolation Forest...\")\n",
    "        contamination = y_train.mean()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        model = IsolationForest(contamination=contamination, random_state=42, n_jobs=-1)\n",
    "        model.fit(X_train)\n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        start_time = time.time()\n",
    "        anomaly_scores = -model.score_samples(X_test)\n",
    "        inference_time = time.time() - start_time\n",
    "        \n",
    "        result = self.evaluator.evaluate_unsupervised(\n",
    "            y_test, anomaly_scores, 'Isolation Forest', dataset_name,\n",
    "            train_time, inference_time, contamination=contamination, imbalance_strategy=imbalance_strategy\n",
    "        )\n",
    "        self.evaluator.print_result(result)\n",
    "        \n",
    "        del model\n",
    "        clear_memory()\n",
    "        return result\n",
    "    \n",
    "    def run_autoencoder(self, X_train, y_train, X_test, y_test, dataset_name, imbalance_strategy='none'):\n",
    "        \"\"\"AutoencoderÔºàÊó†ÁõëÁù£ÔºåÂè™withNormalsamplesÔºâ\"\"\"\n",
    "        print(f\"\\nüöÄ Training Autoencoder...\")\n",
    "        X_train_normal = X_train[y_train == 0]\n",
    "        print(f\"   Using {len(X_train_normal):,} NormalsamplesTraining\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        model = Autoencoder(input_dim=X_train.shape[1], encoding_dims=[128, 64, 32])\n",
    "        model = train_autoencoder(model, X_train_normal, epochs=50, use_multi_gpu=True)\n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        # ËÆ°ÁÆóÈáçÊûÑËØØÂ∑Æ\n",
    "        model.eval()\n",
    "        start_time = time.time()\n",
    "        X_test_tensor = torch.FloatTensor(X_test.values).to(self.device)\n",
    "        if isinstance(model, nn.DataParallel):\n",
    "            anomaly_scores = model.module.get_reconstruction_error(X_test_tensor)\n",
    "        else:\n",
    "            anomaly_scores = model.get_reconstruction_error(X_test_tensor)\n",
    "        inference_time = time.time() - start_time\n",
    "        \n",
    "        result = self.evaluator.evaluate_unsupervised(\n",
    "            y_test, anomaly_scores, 'Autoencoder', dataset_name,\n",
    "            train_time, inference_time, contamination=y_train.mean(), imbalance_strategy=imbalance_strategy\n",
    "        )\n",
    "        self.evaluator.print_result(result)\n",
    "        \n",
    "        del model\n",
    "        clear_memory()\n",
    "        return result\n",
    "    \n",
    "    def run_all_models(self, X_train, y_train, X_test, y_test, dataset_name, skip_slow=False):\n",
    "        \"\"\"running all modelsÔºàÊîØÊåÅclass imbalancecomparisonÔºâ\"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"üî¨ StartinginDataset '{dataset_name}' running all models\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Á°ÆÂÆöË¶ÅUsingofImbalance strategy\n",
    "        if self.compare_imbalance:\n",
    "            strategies = ['none', 'smote']  # comparisonÔºöNone vs SMOTE\n",
    "            print(f\"üìä Â∞Ücomparisonimbalance handlingÊñπÊ≥ï: {[IMBALANCE_STRATEGIES[s]['name'] for s in strategies]}\")\n",
    "        else:\n",
    "            strategies = ['none']\n",
    "        \n",
    "        models = {}\n",
    "        \n",
    "        for strategy in strategies:\n",
    "            print(f\"\\n{'‚îÄ'*80}\")\n",
    "            print(f\"üìå Imbalance strategy: {IMBALANCE_STRATEGIES[strategy]['name']}\")\n",
    "            print(f\"{'‚îÄ'*80}\")\n",
    "            \n",
    "            # Âø´ÈÄüModelÔºàÊîØÊåÅÂ§ßDatasetÔºâ\n",
    "            models[f'lr_{strategy}'] = self.run_logistic_regression(X_train, y_train, X_test, y_test, dataset_name, strategy)\n",
    "            models[f'rf_{strategy}'] = self.run_random_forest(X_train, y_train, X_test, y_test, dataset_name, strategy)\n",
    "            models[f'xgb_{strategy}'] = self.run_xgboost(X_train, y_train, X_test, y_test, dataset_name, strategy)\n",
    "            models[f'lgb_{strategy}'] = self.run_lightgbm(X_train, y_train, X_test, y_test, dataset_name, strategy)\n",
    "            models[f'mlp_{strategy}'] = self.run_mlp(X_train, y_train, X_test, y_test, dataset_name, strategy)\n",
    "            \n",
    "            # ÊÖ¢ÈÄüModelÔºàÂè™inÂ∞èDatasetorÈááÊ†∑ÂêéËøêË°åÔºâ\n",
    "            if not skip_slow or len(X_train) < 30000:\n",
    "                models[f'knn_{strategy}'] = self.run_knn(X_train, y_train, X_test, y_test, dataset_name, strategy)\n",
    "                models[f'pca_svm_{strategy}'] = self.run_pca_svm(X_train, y_train, X_test, y_test, dataset_name, strategy)\n",
    "            else:\n",
    "                print(f\"\\n‚ö†Ô∏è  Large dataset, skipping KNN and PCA+SVM or using sampling\")\n",
    "                if self.use_sampling_for_slow_models:\n",
    "                    models[f'knn_{strategy}'] = self.run_knn(X_train, y_train, X_test, y_test, dataset_name, strategy)\n",
    "                    models[f'pca_svm_{strategy}'] = self.run_pca_svm(X_train, y_train, X_test, y_test, dataset_name, strategy)\n",
    "        \n",
    "        # # Unsupervised models (run once, not affected by imbalance handling)\n",
    "        models['isolation_forest'] = self.run_isolation_forest(X_train, y_train, X_test, y_test, dataset_name, 'none')\n",
    "        models['autoencoder'] = self.run_autoencoder(X_train, y_train, X_test, y_test, dataset_name, 'none')\n",
    "        \n",
    "        return models\n",
    "\n",
    "\n",
    "print(\"‚úÖ ExperimentRunner ÊûÑÂª∫completedÔºàÊîØÊåÅimbalancecomparisonÔºâÔºÅ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fcb8cd",
   "metadata": {},
   "source": [
    "## 5. ÁªìÊûúÂèØËßÜÂåñandÂàÜÊûê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03a5a14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ÂèØËßÜÂåñÊ®°ÂùóÊûÑÂª∫completedÔºàÊîØÊåÅimbalancecomparisonÔºâÔºÅ\n"
     ]
    }
   ],
   "source": [
    "class ResultsAnalyzer:\n",
    "    \"\"\"Results Analysis and Visualization (supports imbalance comparison)\"\"\"\n",
    "    \n",
    "    def __init__(self, results_df):\n",
    "        self.results_df = results_df\n",
    "    \n",
    "    def plot_imbalance_comparison(self, metric='f1_score', figsize=(16, 10)):\n",
    "        \"\"\"comparison‰∏çÂêåimbalance handlingÊñπÊ≥ïofÊïàÊûú\"\"\"\n",
    "        if 'imbalance_strategy' not in self.results_df.columns:\n",
    "            print(\"‚ö†Ô∏è  No imbalance comparison experiment conducted\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        strategies = self.results_df['imbalance_strategy'].unique()\n",
    "        datasets = self.results_df['dataset'].unique()\n",
    "        \n",
    "        # 1. ÊØèModelin‰∏çÂêåstrategy‰∏ãofË°®Áé∞\n",
    "        ax = axes[0]\n",
    "        models = self.results_df['model'].unique()\n",
    "        x = np.arange(len(models))\n",
    "        width = 0.8 / len(strategies)\n",
    "        \n",
    "        for i, strategy in enumerate(strategies):\n",
    "            strategy_data = self.results_df[self.results_df['imbalance_strategy'] == strategy]\n",
    "            means = [strategy_data[strategy_data['model'] == m][metric].mean() for m in models]\n",
    "            ax.bar(x + i * width, means, width, label=strategy, alpha=0.8)\n",
    "        \n",
    "        ax.set_xlabel('Model', fontsize=12)\n",
    "        ax.set_ylabel(f'Average {metric.upper()}', fontsize=12)\n",
    "        ax.set_title('‰∏çÂêåImbalance strategyforÂêÑModelofÂΩ±Âìç', fontsize=14, fontweight='bold')\n",
    "        ax.set_xticks(x + width * (len(strategies) - 1) / 2)\n",
    "        ax.set_xticklabels(models, rotation=45, ha='right')\n",
    "        ax.legend(title='strategy')\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # 2. ÊØèDatasetin‰∏çÂêåstrategy‰∏ãofË°®Áé∞\n",
    "        ax = axes[1]\n",
    "        x = np.arange(len(datasets))\n",
    "        \n",
    "        for i, strategy in enumerate(strategies):\n",
    "            strategy_data = self.results_df[self.results_df['imbalance_strategy'] == strategy]\n",
    "            means = [strategy_data[strategy_data['dataset'] == d][metric].mean() for d in datasets]\n",
    "            ax.bar(x + i * width, means, width, label=strategy, alpha=0.8)\n",
    "        \n",
    "        ax.set_xlabel('Dataset', fontsize=12)\n",
    "        ax.set_ylabel(f'Average {metric.upper()}', fontsize=12)\n",
    "        ax.set_title('‰∏çÂêåImbalance strategyinÂêÑDataset‰∏äofÊïàÊûú', fontsize=14, fontweight='bold')\n",
    "        ax.set_xticks(x + width * (len(strategies) - 1) / 2)\n",
    "        ax.set_xticklabels(datasets, rotation=45, ha='right')\n",
    "        ax.legend(title='strategy')\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # 3. Strategy improvement effect (relative to none)\n",
    "        ax = axes[2]\n",
    "        if 'none' in strategies and len(strategies) > 1:\n",
    "            improvements = []\n",
    "            labels = []\n",
    "            \n",
    "            for strategy in strategies:\n",
    "                if strategy != 'none':\n",
    "                    none_scores = self.results_df[self.results_df['imbalance_strategy'] == 'none'][metric].mean()\n",
    "                    strategy_scores = self.results_df[self.results_df['imbalance_strategy'] == strategy][metric].mean()\n",
    "                    improvement = ((strategy_scores - none_scores) / none_scores) * 100\n",
    "                    improvements.append(improvement)\n",
    "                    labels.append(strategy)\n",
    "            \n",
    "            colors = ['green' if x > 0 else 'red' for x in improvements]\n",
    "            ax.barh(labels, improvements, color=colors, alpha=0.7)\n",
    "            ax.set_xlabel('F1-Score Improvement (%)', fontsize=12)\n",
    "            ax.set_title('Áõ∏for‰∫éNoneofÊÄßËÉΩImprovement', fontsize=14, fontweight='bold')\n",
    "            ax.axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
    "            ax.grid(axis='x', alpha=0.3)\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'Need to compare with none strategy', ha='center', va='center', transform=ax.transAxes)\n",
    "            ax.axis('off')\n",
    "        \n",
    "        # 4. Best Strategy Statistics\n",
    "        ax = axes[3]\n",
    "        best_strategies = []\n",
    "        for dataset in datasets:\n",
    "            dataset_data = self.results_df[self.results_df['dataset'] == dataset]\n",
    "            for model in models:\n",
    "                model_data = dataset_data[dataset_data['model'] == model]\n",
    "                if len(model_data) > 0:\n",
    "                    best_idx = model_data[metric].idxmax()\n",
    "                    best_strategy = model_data.loc[best_idx, 'imbalance_strategy']\n",
    "                    best_strategies.append(best_strategy)\n",
    "        \n",
    "        strategy_counts = pd.Series(best_strategies).value_counts()\n",
    "        ax.pie(strategy_counts.values, labels=strategy_counts.index, autopct='%1.1f%%',\n",
    "              startangle=90, colors=plt.cm.Set3.colors)\n",
    "        ax.set_title('Best Strategy Distribution', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_metric_comparison(self, metric='f1_score', figsize=(14, 6)):\n",
    "        \"\"\"ÊØîËæÉ‰∏çÂêåModelinÂêÑDataset‰∏äofÊåáÊ†á\"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
    "        \n",
    "        # ÊåâDatasetÂàÜÁªÑ\n",
    "        datasets = self.results_df['dataset'].unique()\n",
    "        \n",
    "        # Â∑¶Âõæ: ÂêÑModelin‰∏çÂêåDataset‰∏äofË°®Áé∞\n",
    "        pivot_data = self.results_df.pivot_table(\n",
    "            index='model', columns='dataset', values=metric, aggfunc='mean'\n",
    "        )\n",
    "        pivot_data.plot(kind='bar', ax=axes[0])\n",
    "        axes[0].set_title(f'{metric.upper()} - ÊåâModelÂàÜÁªÑ', fontsize=14, fontweight='bold')\n",
    "        axes[0].set_xlabel('Model', fontsize=12)\n",
    "        axes[0].set_ylabel(metric.upper(), fontsize=12)\n",
    "        axes[0].legend(title='Dataset', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        axes[0].grid(axis='y', alpha=0.3)\n",
    "        axes[0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Âè≥Âõæ: ÂêÑDataset‰∏äModelofAverageË°®Áé∞\n",
    "        avg_by_model = self.results_df.groupby('model')[metric].mean().sort_values(ascending=False)\n",
    "        avg_by_model.plot(kind='barh', ax=axes[1], color='skyblue')\n",
    "        axes[1].set_title(f'{metric.upper()} - ModelAverageË°®Áé∞', fontsize=14, fontweight='bold')\n",
    "        axes[1].set_xlabel(f'Average {metric.upper()}', fontsize=12)\n",
    "        axes[1].set_ylabel('Model', fontsize=12)\n",
    "        axes[1].grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_all_metrics_heatmap(self, figsize=(16, 10)):\n",
    "        \"\"\"ÁªòÂà∂ÊâÄÊúâÊåáÊ†áofÁÉ≠ÂäõÂõæ\"\"\"\n",
    "        metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=figsize)\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for idx, metric in enumerate(metrics):\n",
    "            pivot_data = self.results_df.pivot_table(\n",
    "                index='model', columns='dataset', values=metric, aggfunc='mean'\n",
    "            )\n",
    "            sns.heatmap(pivot_data, annot=True, fmt='.3f', cmap='YlGnBu', \n",
    "                       ax=axes[idx], cbar_kws={'label': metric.upper()})\n",
    "            axes[idx].set_title(f'{metric.upper()}', fontsize=12, fontweight='bold')\n",
    "            axes[idx].set_xlabel('Dataset', fontsize=10)\n",
    "            axes[idx].set_ylabel('Model', fontsize=10)\n",
    "        \n",
    "        # ÈöêËóèÂ§ö‰ΩôofÂ≠êÂõæ\n",
    "        for idx in range(len(metrics), len(axes)):\n",
    "            axes[idx].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_time_comparison(self, figsize=(14, 5)):\n",
    "        \"\"\"ÊØîËæÉTrainingTimeandInference time\"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
    "        \n",
    "        # TrainingTime\n",
    "        avg_train_time = self.results_df.groupby('model')['train_time'].mean().sort_values(ascending=False)\n",
    "        avg_train_time.plot(kind='barh', ax=axes[0], color='coral')\n",
    "        axes[0].set_title('AverageTrainingTime', fontsize=14, fontweight='bold')\n",
    "        axes[0].set_xlabel('Time (seconds)', fontsize=12)\n",
    "        axes[0].set_ylabel('Model', fontsize=12)\n",
    "        axes[0].grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        # Inference time\n",
    "        avg_inference_time = self.results_df.groupby('model')['inference_time'].mean().sort_values(ascending=False)\n",
    "        avg_inference_time.plot(kind='barh', ax=axes[1], color='lightgreen')\n",
    "        axes[1].set_title('AverageInference time', fontsize=14, fontweight='bold')\n",
    "        axes[1].set_xlabel('Time (seconds)', fontsize=12)\n",
    "        axes[1].set_ylabel('Model', fontsize=12)\n",
    "        axes[1].grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_dataset_comparison(self, figsize=(14, 8)):\n",
    "        \"\"\"ÊØîËæÉ‰∏çÂêåDatasetofË°®Áé∞\"\"\"\n",
    "        datasets = self.results_df['dataset'].unique()\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        metrics = ['f1_score', 'roc_auc', 'precision', 'recall']\n",
    "        \n",
    "        for idx, metric in enumerate(metrics):\n",
    "            for dataset in datasets:\n",
    "                dataset_data = self.results_df[self.results_df['dataset'] == dataset]\n",
    "                dataset_data_avg = dataset_data.groupby('model')[metric].mean().sort_values(ascending=False)\n",
    "                axes[idx].plot(range(len(dataset_data_avg)), dataset_data_avg.values, \n",
    "                             marker='o', label=dataset, linewidth=2)\n",
    "            \n",
    "            axes[idx].set_title(f'{metric.upper()} comparison', fontsize=12, fontweight='bold')\n",
    "            axes[idx].set_xlabel('ModelÊéíÂêç', fontsize=10)\n",
    "            axes[idx].set_ylabel(metric.upper(), fontsize=10)\n",
    "            axes[idx].legend(title='Dataset', fontsize=8)\n",
    "            axes[idx].grid(alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def generate_summary_table(self):\n",
    "        \"\"\"Generate summary table\"\"\"\n",
    "        print(\"\\n\" + \"=\"*120)\n",
    "        print(\"üìä Experiment Results Summary\")\n",
    "        print(\"=\"*120 + \"\\n\")\n",
    "        \n",
    "        # ÊåâDatasetÂàÜÁªÑÊòæÁ§∫\n",
    "        for dataset in self.results_df['dataset'].unique():\n",
    "            dataset_results = self.results_df[self.results_df['dataset'] == dataset]\n",
    "            dataset_results_sorted = dataset_results.sort_values('f1_score', ascending=False)\n",
    "            \n",
    "            print(f\"\\nüîπ Dataset: {dataset}\")\n",
    "            print(\"-\" * 120)\n",
    "            \n",
    "            display_df = dataset_results_sorted[[\n",
    "                'model', 'imbalance_strategy', 'accuracy', 'precision', 'recall', 'f1_score', \n",
    "                'roc_auc', 'train_time', 'inference_time'\n",
    "            ]].copy()\n",
    "            \n",
    "            # Ê†ºÂºèÂåñÊï∞Value\n",
    "            for col in ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']:\n",
    "                display_df[col] = display_df[col].apply(lambda x: f'{x:.4f}' if pd.notna(x) else 'N/A')\n",
    "            for col in ['train_time', 'inference_time']:\n",
    "                display_df[col] = display_df[col].apply(lambda x: f'{x:.2f}s' if pd.notna(x) else 'N/A')\n",
    "            \n",
    "            print(display_df.to_string(index=False))\n",
    "            print()\n",
    "        \n",
    "        # ÊÄª‰ΩìBestModel\n",
    "        print(\"\\n\" + \"=\"*120)\n",
    "        print(\"üèÜ BestModelÊÄªÁªì\")\n",
    "        print(\"=\"*120 + \"\\n\")\n",
    "        \n",
    "        metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']\n",
    "        for metric in metrics:\n",
    "            best_result = self.results_df.loc[self.results_df[metric].idxmax()]\n",
    "            print(f\"Best {metric.upper():15s}: {best_result['model']:20s} \"\n",
    "                  f\"(Dataset: {best_result['dataset']:20s}, \"\n",
    "                  f\"strategy: {best_result['imbalance_strategy']:15s}, \"\n",
    "                  f\"Value: {best_result[metric]:.4f})\")\n",
    "        \n",
    "        # imbalanceprocessingcomparison\n",
    "        if 'imbalance_strategy' in self.results_df.columns:\n",
    "            print(\"\\n\" + \"=\"*120)\n",
    "            print(\"üìä imbalance handlingÊïàÊûúcomparison\")\n",
    "            print(\"=\"*120 + \"\\n\")\n",
    "            \n",
    "            strategy_performance = self.results_df.groupby('imbalance_strategy')[['f1_score', 'precision', 'recall', 'roc_auc']].mean()\n",
    "            print(strategy_performance.round(4))\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*120 + \"\\n\")\n",
    "    \n",
    "    def export_results(self, output_path):\n",
    "        \"\"\"Export results to CSV\"\"\"\n",
    "        self.results_df.to_csv(output_path, index=False)\n",
    "        print(f\"‚úÖ Results saved to: {output_path}\")\n",
    "\n",
    "\n",
    "print(\"‚úÖ ÂèØËßÜÂåñÊ®°ÂùóÊûÑÂª∫completedÔºàÊîØÊåÅimbalancecomparisonÔºâÔºÅ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f7d0c3",
   "metadata": {},
   "source": [
    "## 6. ËøêË°åÂÆåÊï¥ÂÆûÈ™å\n",
    "\n",
    "Áé∞inÊàë‰ª¨Â∞ÜinÊâÄÊúâDatasetrunning all modelsÂπ∂ÂàÜÊûêÁªìÊûú„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7620ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting OPTIMIZED experiment with memory-efficient strategy...\n",
      "\n",
      "====================================================================================================\n",
      "Experiment Configuration:\n",
      "  - Compare imbalance handling: Yes\n",
      "  - GPU acceleration: Enabled\n",
      "  - Total datasets: 7\n",
      "  - Small datasets (batch process): 2\n",
      "  - Medium datasets (batch process): 3\n",
      "  - Large datasets (one-by-one): 2\n",
      "====================================================================================================\n",
      "\n",
      "####################################################################################################\n",
      "# PHASE 1: Processing Small & Medium Datasets (Batch Mode)\n",
      "####################################################################################################\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Loading dataset: counterfeit_products\n",
      "================================================================================\n",
      "\n",
      "üìä counterfeit_products - Train: (4000, 16), Test: (1000, 16)\n",
      "\n",
      "Feature Analysis:\n",
      "  - Numerical: 6 features\n",
      "  - Categorical: 7 features\n",
      "  - ID: 1 features (will be removed)\n",
      "  - Timestamp: 1 features (will be removed)\n",
      "\n",
      "Class Imbalance Ratio: 2.40:1\n",
      "Fraud Rate: 29.4000% (1,176 fraud samples)\n",
      "\n",
      "Encoding categorical features: ['hour_of_day', 'day_of_week', 'category', 'brand', 'product_images', 'spelling_errors', 'address_match']\n",
      "\n",
      "‚úÖ Preprocessing complete! Final features: 13\n",
      "   Training set: 4,000 samples\n",
      "   Test set: 1,000 samples\n",
      "   Label distribution - Train: Normal=0.7060, Fraud=0.2940\n",
      "   Label distribution - Test: Normal=0.7060, Fraud=0.2940\n",
      "‚úÖ Data loaded:\n",
      "   Training: 4,000 samples √ó 13 features\n",
      "   Test: 1,000 samples √ó 13 features\n",
      "   Memory usage: üìä Memory Usage: 0.95 GB\n",
      "   GPU 0 - Allocated: 0.00 GB, Reserved: 0.00 GB\n",
      "   GPU 1 - Allocated: 0.00 GB, Reserved: 0.00 GB\n",
      "\n",
      "================================================================================\n",
      "üî¨ StartinginDataset 'counterfeit_products' running all models\n",
      "================================================================================\n",
      "üìä Â∞Ücomparisonimbalance handlingÊñπÊ≥ï: ['None', 'SMOTE']\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "üìå Imbalance strategy: None\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üöÄ Training Logistic Regression [None]...\n",
      "\n",
      "‚úÖ Preprocessing complete! Final features: 13\n",
      "   Training set: 4,000 samples\n",
      "   Test set: 1,000 samples\n",
      "   Label distribution - Train: Normal=0.7060, Fraud=0.2940\n",
      "   Label distribution - Test: Normal=0.7060, Fraud=0.2940\n",
      "‚úÖ Data loaded:\n",
      "   Training: 4,000 samples √ó 13 features\n",
      "   Test: 1,000 samples √ó 13 features\n",
      "   Memory usage: üìä Memory Usage: 0.95 GB\n",
      "   GPU 0 - Allocated: 0.00 GB, Reserved: 0.00 GB\n",
      "   GPU 1 - Allocated: 0.00 GB, Reserved: 0.00 GB\n",
      "\n",
      "================================================================================\n",
      "üî¨ StartinginDataset 'counterfeit_products' running all models\n",
      "================================================================================\n",
      "üìä Â∞Ücomparisonimbalance handlingÊñπÊ≥ï: ['None', 'SMOTE']\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "üìå Imbalance strategy: None\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üöÄ Training Logistic Regression [None]...\n",
      "\n",
      "‚úÖ Logistic Regression completed!\n",
      "   Accuracy: 1.0000\n",
      "   Precision: 1.0000\n",
      "   Recall: 1.0000\n",
      "   F1-Score: 1.0000\n",
      "   ROC-AUC: 1.0000\n",
      "   TrainingTime: 1.57s\n",
      "   Inference time: 0.00s\n",
      "\n",
      "üöÄ Training Random Forest [None]...\n",
      "\n",
      "‚úÖ Logistic Regression completed!\n",
      "   Accuracy: 1.0000\n",
      "   Precision: 1.0000\n",
      "   Recall: 1.0000\n",
      "   F1-Score: 1.0000\n",
      "   ROC-AUC: 1.0000\n",
      "   TrainingTime: 1.57s\n",
      "   Inference time: 0.00s\n",
      "\n",
      "üöÄ Training Random Forest [None]...\n",
      "\n",
      "‚úÖ Random Forest completed!\n",
      "   Accuracy: 1.0000\n",
      "   Precision: 1.0000\n",
      "   Recall: 1.0000\n",
      "   F1-Score: 1.0000\n",
      "   ROC-AUC: 1.0000\n",
      "   TrainingTime: 0.22s\n",
      "   Inference time: 0.08s\n",
      "\n",
      "üöÄ Training XGBoost [None]...\n",
      "\n",
      "‚úÖ Random Forest completed!\n",
      "   Accuracy: 1.0000\n",
      "   Precision: 1.0000\n",
      "   Recall: 1.0000\n",
      "   F1-Score: 1.0000\n",
      "   ROC-AUC: 1.0000\n",
      "   TrainingTime: 0.22s\n",
      "   Inference time: 0.08s\n",
      "\n",
      "üöÄ Training XGBoost [None]...\n",
      "\n",
      "‚úÖ XGBoost completed!\n",
      "   Accuracy: 1.0000\n",
      "   Precision: 1.0000\n",
      "   Recall: 1.0000\n",
      "   F1-Score: 1.0000\n",
      "   ROC-AUC: 1.0000\n",
      "   TrainingTime: 0.63s\n",
      "   Inference time: 0.07s\n",
      "\n",
      "üöÄ Training LightGBM [None]...\n",
      "\n",
      "‚úÖ XGBoost completed!\n",
      "   Accuracy: 1.0000\n",
      "   Precision: 1.0000\n",
      "   Recall: 1.0000\n",
      "   F1-Score: 1.0000\n",
      "   ROC-AUC: 1.0000\n",
      "   TrainingTime: 0.63s\n",
      "   Inference time: 0.07s\n",
      "\n",
      "üöÄ Training LightGBM [None]...\n",
      "\n",
      "‚úÖ LightGBM completed!\n",
      "   Accuracy: 1.0000\n",
      "   Precision: 1.0000\n",
      "   Recall: 1.0000\n",
      "   F1-Score: 1.0000\n",
      "   ROC-AUC: 1.0000\n",
      "   TrainingTime: 0.71s\n",
      "   Inference time: 0.01s\n",
      "\n",
      "üöÄ Training MLP [None]...\n",
      "   Using 2 GPUsfor training\n",
      "\n",
      "‚úÖ LightGBM completed!\n",
      "   Accuracy: 1.0000\n",
      "   Precision: 1.0000\n",
      "   Recall: 1.0000\n",
      "   F1-Score: 1.0000\n",
      "   ROC-AUC: 1.0000\n",
      "   TrainingTime: 0.71s\n",
      "   Inference time: 0.01s\n",
      "\n",
      "üöÄ Training MLP [None]...\n",
      "   Using 2 GPUsfor training\n",
      "   Epoch 10/50 - Train Loss: 0.0649, Val Loss: 0.0604\n",
      "   Epoch 10/50 - Train Loss: 0.0649, Val Loss: 0.0604\n",
      "   Epoch 20/50 - Train Loss: 0.0210, Val Loss: 0.0179\n",
      "   Epoch 20/50 - Train Loss: 0.0210, Val Loss: 0.0179\n",
      "   Epoch 30/50 - Train Loss: 0.0107, Val Loss: 0.0086\n",
      "   Epoch 30/50 - Train Loss: 0.0107, Val Loss: 0.0086\n",
      "   Epoch 40/50 - Train Loss: 0.0064, Val Loss: 0.0057\n",
      "   Epoch 40/50 - Train Loss: 0.0064, Val Loss: 0.0057\n",
      "   Epoch 50/50 - Train Loss: 0.0044, Val Loss: 0.0036\n",
      "\n",
      "‚úÖ MLP completed!\n",
      "   Accuracy: 1.0000\n",
      "   Precision: 1.0000\n",
      "   Recall: 1.0000\n",
      "   F1-Score: 1.0000\n",
      "   ROC-AUC: 1.0000\n",
      "   TrainingTime: 14.28s\n",
      "   Inference time: 0.00s\n",
      "   Epoch 50/50 - Train Loss: 0.0044, Val Loss: 0.0036\n",
      "\n",
      "‚úÖ MLP completed!\n",
      "   Accuracy: 1.0000\n",
      "   Precision: 1.0000\n",
      "   Recall: 1.0000\n",
      "   F1-Score: 1.0000\n",
      "   ROC-AUC: 1.0000\n",
      "   TrainingTime: 14.28s\n",
      "   Inference time: 0.00s\n",
      "\n",
      "üöÄ Training KNN [None]...\n",
      "\n",
      "‚úÖ KNN completed!\n",
      "   Accuracy: 0.9990\n",
      "   Precision: 0.9966\n",
      "   Recall: 1.0000\n",
      "   F1-Score: 0.9983\n",
      "   ROC-AUC: 1.0000\n",
      "   TrainingTime: 0.00s\n",
      "   Inference time: 0.12s\n",
      "\n",
      "üöÄ Training KNN [None]...\n",
      "\n",
      "‚úÖ KNN completed!\n",
      "   Accuracy: 0.9990\n",
      "   Precision: 0.9966\n",
      "   Recall: 1.0000\n",
      "   F1-Score: 0.9983\n",
      "   ROC-AUC: 1.0000\n",
      "   TrainingTime: 0.00s\n",
      "   Inference time: 0.12s\n",
      "\n",
      "üöÄ Training PCA + SVM [None]...\n",
      "   PCAdimension reduction: 13 -> 10 dimensions\n",
      "\n",
      "‚úÖ PCA+SVM completed!\n",
      "   Accuracy: 1.0000\n",
      "   Precision: 1.0000\n",
      "   Recall: 1.0000\n",
      "   F1-Score: 1.0000\n",
      "   ROC-AUC: 1.0000\n",
      "   TrainingTime: 0.10s\n",
      "   Inference time: 0.00s\n",
      "\n",
      "üöÄ Training PCA + SVM [None]...\n",
      "   PCAdimension reduction: 13 -> 10 dimensions\n",
      "\n",
      "‚úÖ PCA+SVM completed!\n",
      "   Accuracy: 1.0000\n",
      "   Precision: 1.0000\n",
      "   Recall: 1.0000\n",
      "   F1-Score: 1.0000\n",
      "   ROC-AUC: 1.0000\n",
      "   TrainingTime: 0.10s\n",
      "   Inference time: 0.00s\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "üìå Imbalance strategy: SMOTE\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üöÄ Training Logistic Regression [SMOTE]...\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "üìå Imbalance strategy: SMOTE\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üöÄ Training Logistic Regression [SMOTE]...\n",
      "\n",
      "‚úÖ Logistic Regression completed!\n",
      "   Accuracy: 1.0000\n",
      "   Precision: 1.0000\n",
      "   Recall: 1.0000\n",
      "   F1-Score: 1.0000\n",
      "   ROC-AUC: 1.0000\n",
      "   TrainingTime: 0.44s\n",
      "   Inference time: 0.00s\n",
      "\n",
      "üöÄ Training Random Forest [SMOTE]...\n",
      "\n",
      "‚úÖ Logistic Regression completed!\n",
      "   Accuracy: 1.0000\n",
      "   Precision: 1.0000\n",
      "   Recall: 1.0000\n",
      "   F1-Score: 1.0000\n",
      "   ROC-AUC: 1.0000\n",
      "   TrainingTime: 0.44s\n",
      "   Inference time: 0.00s\n",
      "\n",
      "üöÄ Training Random Forest [SMOTE]...\n",
      "\n",
      "‚úÖ Random Forest completed!\n",
      "   Accuracy: 1.0000\n",
      "   Precision: 1.0000\n",
      "   Recall: 1.0000\n",
      "   F1-Score: 1.0000\n",
      "   ROC-AUC: 1.0000\n",
      "   TrainingTime: 0.20s\n",
      "   Inference time: 0.08s\n",
      "\n",
      "üöÄ Training XGBoost [SMOTE]...\n",
      "\n",
      "‚úÖ Random Forest completed!\n",
      "   Accuracy: 1.0000\n",
      "   Precision: 1.0000\n",
      "   Recall: 1.0000\n",
      "   F1-Score: 1.0000\n",
      "   ROC-AUC: 1.0000\n",
      "   TrainingTime: 0.20s\n",
      "   Inference time: 0.08s\n",
      "\n",
      "üöÄ Training XGBoost [SMOTE]...\n",
      "\n",
      "‚úÖ XGBoost completed!\n",
      "   Accuracy: 1.0000\n",
      "   Precision: 1.0000\n",
      "   Recall: 1.0000\n",
      "   F1-Score: 1.0000\n",
      "   ROC-AUC: 1.0000\n",
      "   TrainingTime: 0.93s\n",
      "   Inference time: 0.01s\n",
      "\n",
      "üöÄ Training LightGBM [SMOTE]...\n",
      "\n",
      "‚úÖ XGBoost completed!\n",
      "   Accuracy: 1.0000\n",
      "   Precision: 1.0000\n",
      "   Recall: 1.0000\n",
      "   F1-Score: 1.0000\n",
      "   ROC-AUC: 1.0000\n",
      "   TrainingTime: 0.93s\n",
      "   Inference time: 0.01s\n",
      "\n",
      "üöÄ Training LightGBM [SMOTE]...\n",
      "\n",
      "‚úÖ LightGBM completed!\n",
      "   Accuracy: 1.0000\n",
      "   Precision: 1.0000\n",
      "   Recall: 1.0000\n",
      "   F1-Score: 1.0000\n",
      "   ROC-AUC: 1.0000\n",
      "   TrainingTime: 0.29s\n",
      "   Inference time: 0.00s\n",
      "\n",
      "üöÄ Training MLP [SMOTE]...\n",
      "   Using 2 GPUsfor training\n",
      "\n",
      "‚úÖ LightGBM completed!\n",
      "   Accuracy: 1.0000\n",
      "   Precision: 1.0000\n",
      "   Recall: 1.0000\n",
      "   F1-Score: 1.0000\n",
      "   ROC-AUC: 1.0000\n",
      "   TrainingTime: 0.29s\n",
      "   Inference time: 0.00s\n",
      "\n",
      "üöÄ Training MLP [SMOTE]...\n",
      "   Using 2 GPUsfor training\n",
      "   Epoch 10/50 - Train Loss: 0.0271, Val Loss: 0.0216\n",
      "   Epoch 10/50 - Train Loss: 0.0271, Val Loss: 0.0216\n",
      "   Epoch 20/50 - Train Loss: 0.0089, Val Loss: 0.0066\n",
      "   Epoch 20/50 - Train Loss: 0.0089, Val Loss: 0.0066\n",
      "   Epoch 30/50 - Train Loss: 0.0046, Val Loss: 0.0031\n",
      "   Epoch 30/50 - Train Loss: 0.0046, Val Loss: 0.0031\n",
      "   Epoch 40/50 - Train Loss: 0.0028, Val Loss: 0.0018\n",
      "   Epoch 40/50 - Train Loss: 0.0028, Val Loss: 0.0018\n",
      "   Epoch 50/50 - Train Loss: 0.0018, Val Loss: 0.0012\n",
      "\n",
      "‚úÖ MLP completed!\n",
      "   Accuracy: 1.0000\n",
      "   Precision: 1.0000\n",
      "   Recall: 1.0000\n",
      "   F1-Score: 1.0000\n",
      "   ROC-AUC: 1.0000\n",
      "   TrainingTime: 11.00s\n",
      "   Inference time: 0.00s\n",
      "   Epoch 50/50 - Train Loss: 0.0018, Val Loss: 0.0012\n",
      "\n",
      "‚úÖ MLP completed!\n",
      "   Accuracy: 1.0000\n",
      "   Precision: 1.0000\n",
      "   Recall: 1.0000\n",
      "   F1-Score: 1.0000\n",
      "   ROC-AUC: 1.0000\n",
      "   TrainingTime: 11.00s\n",
      "   Inference time: 0.00s\n",
      "\n",
      "üöÄ Training KNN [SMOTE]...\n",
      "\n",
      "‚úÖ KNN completed!\n",
      "   Accuracy: 0.9990\n",
      "   Precision: 0.9966\n",
      "   Recall: 1.0000\n",
      "   F1-Score: 0.9983\n",
      "   ROC-AUC: 1.0000\n",
      "   TrainingTime: 0.01s\n",
      "   Inference time: 0.13s\n",
      "\n",
      "üöÄ Training KNN [SMOTE]...\n",
      "\n",
      "‚úÖ KNN completed!\n",
      "   Accuracy: 0.9990\n",
      "   Precision: 0.9966\n",
      "   Recall: 1.0000\n",
      "   F1-Score: 0.9983\n",
      "   ROC-AUC: 1.0000\n",
      "   TrainingTime: 0.01s\n",
      "   Inference time: 0.13s\n",
      "\n",
      "üöÄ Training PCA + SVM [SMOTE]...\n",
      "   PCAdimension reduction: 13 -> 10 dimensions\n",
      "\n",
      "‚úÖ PCA+SVM completed!\n",
      "   Accuracy: 1.0000\n",
      "   Precision: 1.0000\n",
      "   Recall: 1.0000\n",
      "   F1-Score: 1.0000\n",
      "   ROC-AUC: 1.0000\n",
      "   TrainingTime: 0.08s\n",
      "   Inference time: 0.00s\n",
      "\n",
      "üöÄ Training Isolation Forest...\n",
      "\n",
      "üöÄ Training PCA + SVM [SMOTE]...\n",
      "   PCAdimension reduction: 13 -> 10 dimensions\n",
      "\n",
      "‚úÖ PCA+SVM completed!\n",
      "   Accuracy: 1.0000\n",
      "   Precision: 1.0000\n",
      "   Recall: 1.0000\n",
      "   F1-Score: 1.0000\n",
      "   ROC-AUC: 1.0000\n",
      "   TrainingTime: 0.08s\n",
      "   Inference time: 0.00s\n",
      "\n",
      "üöÄ Training Isolation Forest...\n",
      "\n",
      "‚úÖ Isolation Forest completed!\n",
      "   Accuracy: 0.5960\n",
      "   Precision: 0.3363\n",
      "   Recall: 0.3844\n",
      "   F1-Score: 0.3587\n",
      "   ROC-AUC: 0.5327\n",
      "   TrainingTime: 0.29s\n",
      "   Inference time: 0.02s\n",
      "\n",
      "üöÄ Training Autoencoder...\n",
      "   Using 2,824 NormalsamplesTraining\n",
      "   Using 2 GPUsfor training\n",
      "\n",
      "‚úÖ Isolation Forest completed!\n",
      "   Accuracy: 0.5960\n",
      "   Precision: 0.3363\n",
      "   Recall: 0.3844\n",
      "   F1-Score: 0.3587\n",
      "   ROC-AUC: 0.5327\n",
      "   TrainingTime: 0.29s\n",
      "   Inference time: 0.02s\n",
      "\n",
      "üöÄ Training Autoencoder...\n",
      "   Using 2,824 NormalsamplesTraining\n",
      "   Using 2 GPUsfor training\n",
      "   Epoch 10/50 - Train Loss: 0.2551\n",
      "   Epoch 10/50 - Train Loss: 0.2551\n",
      "   Epoch 20/50 - Train Loss: 0.1921\n",
      "   Epoch 20/50 - Train Loss: 0.1921\n",
      "   Epoch 30/50 - Train Loss: 0.1687\n",
      "   Epoch 30/50 - Train Loss: 0.1687\n",
      "   Epoch 40/50 - Train Loss: 0.1610\n",
      "   Epoch 40/50 - Train Loss: 0.1610\n",
      "   Epoch 50/50 - Train Loss: 0.1518\n",
      "\n",
      "‚úÖ Autoencoder completed!\n",
      "   Accuracy: 0.9940\n",
      "   Precision: 0.9832\n",
      "   Recall: 0.9966\n",
      "   F1-Score: 0.9899\n",
      "   ROC-AUC: 0.9999\n",
      "   TrainingTime: 14.10s\n",
      "   Inference time: 0.03s\n",
      "   Epoch 50/50 - Train Loss: 0.1518\n",
      "\n",
      "‚úÖ Autoencoder completed!\n",
      "   Accuracy: 0.9940\n",
      "   Precision: 0.9832\n",
      "   Recall: 0.9966\n",
      "   F1-Score: 0.9899\n",
      "   ROC-AUC: 0.9999\n",
      "   TrainingTime: 14.10s\n",
      "   Inference time: 0.03s\n",
      "\n",
      "‚úÖ counterfeit_products completed and cleared from memory\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Loading dataset: counterfeit_transactions\n",
      "================================================================================\n",
      "\n",
      "üìä counterfeit_transactions - Train: (2400, 19), Test: (600, 19)\n",
      "\n",
      "Feature Analysis:\n",
      "  - Numerical: 7 features\n",
      "  - Categorical: 8 features\n",
      "  - ID: 1 features (will be removed)\n",
      "  - Timestamp: 2 features (will be removed)\n",
      "\n",
      "Class Imbalance Ratio: 3.10:1\n",
      "Fraud Rate: 24.4167% (586 fraud samples)\n",
      "\n",
      "Encoding categorical features: ['day_of_week', 'customer_location', 'payment_method', 'quantity', 'discount_applied', 'refund_requested', 'velocity_flag', 'address_match']\n",
      "\n",
      "‚úÖ counterfeit_products completed and cleared from memory\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Loading dataset: counterfeit_transactions\n",
      "================================================================================\n",
      "\n",
      "üìä counterfeit_transactions - Train: (2400, 19), Test: (600, 19)\n",
      "\n",
      "Feature Analysis:\n",
      "  - Numerical: 7 features\n",
      "  - Categorical: 8 features\n",
      "  - ID: 1 features (will be removed)\n",
      "  - Timestamp: 2 features (will be removed)\n",
      "\n",
      "Class Imbalance Ratio: 3.10:1\n",
      "Fraud Rate: 24.4167% (586 fraud samples)\n",
      "\n",
      "Encoding categorical features: ['day_of_week', 'customer_location', 'payment_method', 'quantity', 'discount_applied', 'refund_requested', 'velocity_flag', 'address_match']\n",
      "\n",
      "‚úÖ Preprocessing complete! Final features: 15\n",
      "   Training set: 2,400 samples\n",
      "   Test set: 600 samples\n",
      "   Label distribution - Train: Normal=0.7558, Fraud=0.2442\n",
      "   Label distribution - Test: Normal=0.7550, Fraud=0.2450\n",
      "‚úÖ Data loaded:\n",
      "   Training: 2,400 samples √ó 15 features\n",
      "   Test: 600 samples √ó 15 features\n",
      "   Memory usage: üìä Memory Usage: 1.74 GB\n",
      "   GPU 0 - Allocated: 0.03 GB, Reserved: 0.04 GB\n",
      "   GPU 1 - Allocated: 0.02 GB, Reserved: 0.02 GB\n",
      "\n",
      "================================================================================\n",
      "üî¨ StartinginDataset 'counterfeit_transactions' running all models\n",
      "================================================================================\n",
      "üìä Â∞Ücomparisonimbalance handlingÊñπÊ≥ï: ['None', 'SMOTE']\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "üìå Imbalance strategy: None\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üöÄ Training Logistic Regression [None]...\n",
      "\n",
      "‚úÖ Preprocessing complete! Final features: 15\n",
      "   Training set: 2,400 samples\n",
      "   Test set: 600 samples\n",
      "   Label distribution - Train: Normal=0.7558, Fraud=0.2442\n",
      "   Label distribution - Test: Normal=0.7550, Fraud=0.2450\n",
      "‚úÖ Data loaded:\n",
      "   Training: 2,400 samples √ó 15 features\n",
      "   Test: 600 samples √ó 15 features\n",
      "   Memory usage: üìä Memory Usage: 1.74 GB\n",
      "   GPU 0 - Allocated: 0.03 GB, Reserved: 0.04 GB\n",
      "   GPU 1 - Allocated: 0.02 GB, Reserved: 0.02 GB\n",
      "\n",
      "================================================================================\n",
      "üî¨ StartinginDataset 'counterfeit_transactions' running all models\n",
      "================================================================================\n",
      "üìä Â∞Ücomparisonimbalance handlingÊñπÊ≥ï: ['None', 'SMOTE']\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "üìå Imbalance strategy: None\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üöÄ Training Logistic Regression [None]...\n",
      "\n",
      "‚úÖ Logistic Regression completed!\n",
      "   Accuracy: 0.9117\n",
      "   Precision: 0.7733\n",
      "   Recall: 0.9048\n",
      "   F1-Score: 0.8339\n",
      "   ROC-AUC: 0.9825\n",
      "   TrainingTime: 0.42s\n",
      "   Inference time: 0.00s\n",
      "\n",
      "üöÄ Training Random Forest [None]...\n",
      "\n",
      "‚úÖ Logistic Regression completed!\n",
      "   Accuracy: 0.9117\n",
      "   Precision: 0.7733\n",
      "   Recall: 0.9048\n",
      "   F1-Score: 0.8339\n",
      "   ROC-AUC: 0.9825\n",
      "   TrainingTime: 0.42s\n",
      "   Inference time: 0.00s\n",
      "\n",
      "üöÄ Training Random Forest [None]...\n",
      "\n",
      "‚úÖ Random Forest completed!\n",
      "   Accuracy: 0.9633\n",
      "   Precision: 0.9433\n",
      "   Recall: 0.9048\n",
      "   F1-Score: 0.9236\n",
      "   ROC-AUC: 0.9929\n",
      "   TrainingTime: 0.19s\n",
      "   Inference time: 0.07s\n",
      "\n",
      "üöÄ Training XGBoost [None]...\n",
      "\n",
      "‚úÖ Random Forest completed!\n",
      "   Accuracy: 0.9633\n",
      "   Precision: 0.9433\n",
      "   Recall: 0.9048\n",
      "   F1-Score: 0.9236\n",
      "   ROC-AUC: 0.9929\n",
      "   TrainingTime: 0.19s\n",
      "   Inference time: 0.07s\n",
      "\n",
      "üöÄ Training XGBoost [None]...\n",
      "\n",
      "‚úÖ XGBoost completed!\n",
      "   Accuracy: 0.9750\n",
      "   Precision: 0.9714\n",
      "   Recall: 0.9252\n",
      "   F1-Score: 0.9477\n",
      "   ROC-AUC: 0.9945\n",
      "   TrainingTime: 1.64s\n",
      "   Inference time: 0.06s\n",
      "\n",
      "üöÄ Training LightGBM [None]...\n",
      "\n",
      "‚úÖ XGBoost completed!\n",
      "   Accuracy: 0.9750\n",
      "   Precision: 0.9714\n",
      "   Recall: 0.9252\n",
      "   F1-Score: 0.9477\n",
      "   ROC-AUC: 0.9945\n",
      "   TrainingTime: 1.64s\n",
      "   Inference time: 0.06s\n",
      "\n",
      "üöÄ Training LightGBM [None]...\n",
      "\n",
      "‚úÖ LightGBM completed!\n",
      "   Accuracy: 0.9717\n",
      "   Precision: 0.9643\n",
      "   Recall: 0.9184\n",
      "   F1-Score: 0.9408\n",
      "   ROC-AUC: 0.9946\n",
      "   TrainingTime: 1.63s\n",
      "   Inference time: 0.01s\n",
      "\n",
      "üöÄ Training MLP [None]...\n",
      "   Using 2 GPUsfor training\n",
      "\n",
      "‚úÖ LightGBM completed!\n",
      "   Accuracy: 0.9717\n",
      "   Precision: 0.9643\n",
      "   Recall: 0.9184\n",
      "   F1-Score: 0.9408\n",
      "   ROC-AUC: 0.9946\n",
      "   TrainingTime: 1.63s\n",
      "   Inference time: 0.01s\n",
      "\n",
      "üöÄ Training MLP [None]...\n",
      "   Using 2 GPUsfor training\n",
      "   Epoch 10/50 - Train Loss: 0.3903, Val Loss: 0.3846\n",
      "   Epoch 10/50 - Train Loss: 0.3903, Val Loss: 0.3846\n",
      "   Epoch 20/50 - Train Loss: 0.2974, Val Loss: 0.3224\n",
      "   Epoch 20/50 - Train Loss: 0.2974, Val Loss: 0.3224\n",
      "   Epoch 30/50 - Train Loss: 0.2549, Val Loss: 0.3183\n",
      "   Epoch 30/50 - Train Loss: 0.2549, Val Loss: 0.3183\n",
      "   Early stopping‰∫éepoch 38\n",
      "\n",
      "‚úÖ MLP completed!\n",
      "   Accuracy: 0.9200\n",
      "   Precision: 0.8075\n",
      "   Recall: 0.8844\n",
      "   F1-Score: 0.8442\n",
      "   ROC-AUC: 0.9746\n",
      "   TrainingTime: 8.82s\n",
      "   Inference time: 0.00s\n",
      "   Early stopping‰∫éepoch 38\n",
      "\n",
      "‚úÖ MLP completed!\n",
      "   Accuracy: 0.9200\n",
      "   Precision: 0.8075\n",
      "   Recall: 0.8844\n",
      "   F1-Score: 0.8442\n",
      "   ROC-AUC: 0.9746\n",
      "   TrainingTime: 8.82s\n",
      "   Inference time: 0.00s\n",
      "\n",
      "üöÄ Training KNN [None]...\n",
      "\n",
      "‚úÖ KNN completed!\n",
      "   Accuracy: 0.8883\n",
      "   Precision: 0.8846\n",
      "   Recall: 0.6259\n",
      "   F1-Score: 0.7331\n",
      "   ROC-AUC: 0.8928\n",
      "   TrainingTime: 0.00s\n",
      "   Inference time: 0.11s\n",
      "\n",
      "üöÄ Training KNN [None]...\n",
      "\n",
      "‚úÖ KNN completed!\n",
      "   Accuracy: 0.8883\n",
      "   Precision: 0.8846\n",
      "   Recall: 0.6259\n",
      "   F1-Score: 0.7331\n",
      "   ROC-AUC: 0.8928\n",
      "   TrainingTime: 0.00s\n",
      "   Inference time: 0.11s\n",
      "\n",
      "üöÄ Training PCA + SVM [None]...\n",
      "   PCAdimension reduction: 15 -> 12 dimensions\n",
      "\n",
      "üöÄ Training PCA + SVM [None]...\n",
      "   PCAdimension reduction: 15 -> 12 dimensions\n",
      "\n",
      "‚úÖ PCA+SVM completed!\n",
      "   Accuracy: 0.8300\n",
      "   Precision: 0.6190\n",
      "   Recall: 0.7959\n",
      "   F1-Score: 0.6964\n",
      "   ROC-AUC: 0.8912\n",
      "   TrainingTime: 0.44s\n",
      "   Inference time: 0.04s\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "üìå Imbalance strategy: SMOTE\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üöÄ Training Logistic Regression [SMOTE]...\n",
      "\n",
      "‚úÖ PCA+SVM completed!\n",
      "   Accuracy: 0.8300\n",
      "   Precision: 0.6190\n",
      "   Recall: 0.7959\n",
      "   F1-Score: 0.6964\n",
      "   ROC-AUC: 0.8912\n",
      "   TrainingTime: 0.44s\n",
      "   Inference time: 0.04s\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "üìå Imbalance strategy: SMOTE\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üöÄ Training Logistic Regression [SMOTE]...\n",
      "\n",
      "‚úÖ Logistic Regression completed!\n",
      "   Accuracy: 0.9167\n",
      "   Precision: 0.7870\n",
      "   Recall: 0.9048\n",
      "   F1-Score: 0.8418\n",
      "   ROC-AUC: 0.9827\n",
      "   TrainingTime: 0.43s\n",
      "   Inference time: 0.00s\n",
      "\n",
      "üöÄ Training Random Forest [SMOTE]...\n",
      "\n",
      "‚úÖ Logistic Regression completed!\n",
      "   Accuracy: 0.9167\n",
      "   Precision: 0.7870\n",
      "   Recall: 0.9048\n",
      "   F1-Score: 0.8418\n",
      "   ROC-AUC: 0.9827\n",
      "   TrainingTime: 0.43s\n",
      "   Inference time: 0.00s\n",
      "\n",
      "üöÄ Training Random Forest [SMOTE]...\n",
      "\n",
      "‚úÖ Random Forest completed!\n",
      "   Accuracy: 0.9633\n",
      "   Precision: 0.9433\n",
      "   Recall: 0.9048\n",
      "   F1-Score: 0.9236\n",
      "   ROC-AUC: 0.9931\n",
      "   TrainingTime: 0.20s\n",
      "   Inference time: 0.08s\n",
      "\n",
      "üöÄ Training XGBoost [SMOTE]...\n",
      "\n",
      "‚úÖ Random Forest completed!\n",
      "   Accuracy: 0.9633\n",
      "   Precision: 0.9433\n",
      "   Recall: 0.9048\n",
      "   F1-Score: 0.9236\n",
      "   ROC-AUC: 0.9931\n",
      "   TrainingTime: 0.20s\n",
      "   Inference time: 0.08s\n",
      "\n",
      "üöÄ Training XGBoost [SMOTE]...\n",
      "\n",
      "‚úÖ XGBoost completed!\n",
      "   Accuracy: 0.9750\n",
      "   Precision: 0.9648\n",
      "   Recall: 0.9320\n",
      "   F1-Score: 0.9481\n",
      "   ROC-AUC: 0.9949\n",
      "   TrainingTime: 1.01s\n",
      "   Inference time: 0.01s\n",
      "\n",
      "üöÄ Training LightGBM [SMOTE]...\n",
      "\n",
      "‚úÖ XGBoost completed!\n",
      "   Accuracy: 0.9750\n",
      "   Precision: 0.9648\n",
      "   Recall: 0.9320\n",
      "   F1-Score: 0.9481\n",
      "   ROC-AUC: 0.9949\n",
      "   TrainingTime: 1.01s\n",
      "   Inference time: 0.01s\n",
      "\n",
      "üöÄ Training LightGBM [SMOTE]...\n",
      "\n",
      "‚úÖ LightGBM completed!\n",
      "   Accuracy: 0.9667\n",
      "   Precision: 0.9504\n",
      "   Recall: 0.9116\n",
      "   F1-Score: 0.9306\n",
      "   ROC-AUC: 0.9952\n",
      "   TrainingTime: 0.45s\n",
      "   Inference time: 0.00s\n",
      "\n",
      "üöÄ Training MLP [SMOTE]...\n",
      "   Using 2 GPUsfor training\n",
      "\n",
      "‚úÖ LightGBM completed!\n",
      "   Accuracy: 0.9667\n",
      "   Precision: 0.9504\n",
      "   Recall: 0.9116\n",
      "   F1-Score: 0.9306\n",
      "   ROC-AUC: 0.9952\n",
      "   TrainingTime: 0.45s\n",
      "   Inference time: 0.00s\n",
      "\n",
      "üöÄ Training MLP [SMOTE]...\n",
      "   Using 2 GPUsfor training\n",
      "   Epoch 10/50 - Train Loss: 0.2028, Val Loss: 0.1916\n",
      "   Epoch 10/50 - Train Loss: 0.2028, Val Loss: 0.1916\n",
      "   Epoch 20/50 - Train Loss: 0.1600, Val Loss: 0.1507\n",
      "   Epoch 20/50 - Train Loss: 0.1600, Val Loss: 0.1507\n",
      "   Epoch 30/50 - Train Loss: 0.1273, Val Loss: 0.1344\n",
      "   Epoch 30/50 - Train Loss: 0.1273, Val Loss: 0.1344\n",
      "   Epoch 40/50 - Train Loss: 0.1075, Val Loss: 0.1233\n",
      "   Epoch 40/50 - Train Loss: 0.1075, Val Loss: 0.1233\n",
      "   Epoch 50/50 - Train Loss: 0.0973, Val Loss: 0.1107\n",
      "\n",
      "‚úÖ MLP completed!\n",
      "   Accuracy: 0.9283\n",
      "   Precision: 0.8467\n",
      "   Recall: 0.8639\n",
      "   F1-Score: 0.8552\n",
      "   ROC-AUC: 0.9788\n",
      "   TrainingTime: 12.95s\n",
      "   Inference time: 0.00s\n",
      "   Epoch 50/50 - Train Loss: 0.0973, Val Loss: 0.1107\n",
      "\n",
      "‚úÖ MLP completed!\n",
      "   Accuracy: 0.9283\n",
      "   Precision: 0.8467\n",
      "   Recall: 0.8639\n",
      "   F1-Score: 0.8552\n",
      "   ROC-AUC: 0.9788\n",
      "   TrainingTime: 12.95s\n",
      "   Inference time: 0.00s\n",
      "\n",
      "üöÄ Training KNN [SMOTE]...\n",
      "\n",
      "‚úÖ KNN completed!\n",
      "   Accuracy: 0.8483\n",
      "   Precision: 0.6489\n",
      "   Recall: 0.8299\n",
      "   F1-Score: 0.7284\n",
      "   ROC-AUC: 0.8871\n",
      "   TrainingTime: 0.00s\n",
      "   Inference time: 0.13s\n",
      "\n",
      "üöÄ Training KNN [SMOTE]...\n",
      "\n",
      "‚úÖ KNN completed!\n",
      "   Accuracy: 0.8483\n",
      "   Precision: 0.6489\n",
      "   Recall: 0.8299\n",
      "   F1-Score: 0.7284\n",
      "   ROC-AUC: 0.8871\n",
      "   TrainingTime: 0.00s\n",
      "   Inference time: 0.13s\n",
      "\n",
      "üöÄ Training PCA + SVM [SMOTE]...\n",
      "   PCAdimension reduction: 15 -> 12 dimensions\n",
      "\n",
      "üöÄ Training PCA + SVM [SMOTE]...\n",
      "   PCAdimension reduction: 15 -> 12 dimensions\n",
      "\n",
      "‚úÖ PCA+SVM completed!\n",
      "   Accuracy: 0.8350\n",
      "   Precision: 0.6250\n",
      "   Recall: 0.8163\n",
      "   F1-Score: 0.7080\n",
      "   ROC-AUC: 0.8895\n",
      "   TrainingTime: 0.79s\n",
      "   Inference time: 0.05s\n",
      "\n",
      "üöÄ Training Isolation Forest...\n",
      "\n",
      "‚úÖ PCA+SVM completed!\n",
      "   Accuracy: 0.8350\n",
      "   Precision: 0.6250\n",
      "   Recall: 0.8163\n",
      "   F1-Score: 0.7080\n",
      "   ROC-AUC: 0.8895\n",
      "   TrainingTime: 0.79s\n",
      "   Inference time: 0.05s\n",
      "\n",
      "üöÄ Training Isolation Forest...\n",
      "\n",
      "‚úÖ Isolation Forest completed!\n",
      "   Accuracy: 0.6900\n",
      "   Precision: 0.4126\n",
      "   Recall: 0.6259\n",
      "   F1-Score: 0.4973\n",
      "   ROC-AUC: 0.7126\n",
      "   TrainingTime: 0.26s\n",
      "   Inference time: 0.02s\n",
      "\n",
      "üöÄ Training Autoencoder...\n",
      "   Using 1,814 NormalsamplesTraining\n",
      "   Using 2 GPUsfor training\n",
      "\n",
      "‚úÖ Isolation Forest completed!\n",
      "   Accuracy: 0.6900\n",
      "   Precision: 0.4126\n",
      "   Recall: 0.6259\n",
      "   F1-Score: 0.4973\n",
      "   ROC-AUC: 0.7126\n",
      "   TrainingTime: 0.26s\n",
      "   Inference time: 0.02s\n",
      "\n",
      "üöÄ Training Autoencoder...\n",
      "   Using 1,814 NormalsamplesTraining\n",
      "   Using 2 GPUsfor training\n",
      "   Epoch 10/50 - Train Loss: 0.4704\n",
      "   Epoch 10/50 - Train Loss: 0.4704\n",
      "   Epoch 20/50 - Train Loss: 0.3437\n",
      "   Epoch 20/50 - Train Loss: 0.3437\n",
      "   Epoch 30/50 - Train Loss: 0.2920\n",
      "   Epoch 30/50 - Train Loss: 0.2920\n",
      "   Epoch 40/50 - Train Loss: 0.2702\n",
      "   Epoch 40/50 - Train Loss: 0.2702\n",
      "   Epoch 50/50 - Train Loss: 0.2503\n",
      "\n",
      "‚úÖ Autoencoder completed!\n",
      "   Accuracy: 0.8667\n",
      "   Precision: 0.7219\n",
      "   Recall: 0.7415\n",
      "   F1-Score: 0.7315\n",
      "   ROC-AUC: 0.8909\n",
      "   TrainingTime: 13.78s\n",
      "   Inference time: 0.00s\n",
      "\n",
      "‚úÖ counterfeit_transactions completed and cleared from memory\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Loading dataset: creditCardPCA\n",
      "================================================================================\n",
      "\n",
      "   Epoch 50/50 - Train Loss: 0.2503\n",
      "\n",
      "‚úÖ Autoencoder completed!\n",
      "   Accuracy: 0.8667\n",
      "   Precision: 0.7219\n",
      "   Recall: 0.7415\n",
      "   F1-Score: 0.7315\n",
      "   ROC-AUC: 0.8909\n",
      "   TrainingTime: 13.78s\n",
      "   Inference time: 0.00s\n",
      "\n",
      "‚úÖ counterfeit_transactions completed and cleared from memory\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Loading dataset: creditCardPCA\n",
      "================================================================================\n",
      "\n",
      "üìä creditCardPCA - Train: (227845, 34), Test: (56962, 34)\n",
      "\n",
      "Feature Analysis:\n",
      "  - Numerical: 31 features\n",
      "  - Categorical: 1 features\n",
      "  - ID: 0 features (will be removed)\n",
      "  - Timestamp: 1 features (will be removed)\n",
      "\n",
      "Class Imbalance Ratio: 577.29:1\n",
      "Fraud Rate: 0.1729% (394 fraud samples)\n",
      "\n",
      "Encoding categorical features: ['day_of_week']\n",
      "üìä creditCardPCA - Train: (227845, 34), Test: (56962, 34)\n",
      "\n",
      "Feature Analysis:\n",
      "  - Numerical: 31 features\n",
      "  - Categorical: 1 features\n",
      "  - ID: 0 features (will be removed)\n",
      "  - Timestamp: 1 features (will be removed)\n",
      "\n",
      "Class Imbalance Ratio: 577.29:1\n",
      "Fraud Rate: 0.1729% (394 fraud samples)\n",
      "\n",
      "Encoding categorical features: ['day_of_week']\n",
      "\n",
      "‚úÖ Preprocessing complete! Final features: 32\n",
      "   Training set: 227,845 samples\n",
      "   Test set: 56,962 samples\n",
      "   Label distribution - Train: Normal=0.9983, Fraud=0.0017\n",
      "   Label distribution - Test: Normal=0.9983, Fraud=0.0017\n",
      "‚úÖ Data loaded:\n",
      "   Training: 227,845 samples √ó 32 features\n",
      "   Test: 56,962 samples √ó 32 features\n",
      "   Memory usage: üìä Memory Usage: 2.00 GB\n",
      "   GPU 0 - Allocated: 0.03 GB, Reserved: 0.04 GB\n",
      "   GPU 1 - Allocated: 0.02 GB, Reserved: 0.02 GB\n",
      "\n",
      "================================================================================\n",
      "üî¨ StartinginDataset 'creditCardPCA' running all models\n",
      "================================================================================\n",
      "üìä Â∞Ücomparisonimbalance handlingÊñπÊ≥ï: ['None', 'SMOTE']\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "üìå Imbalance strategy: None\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üöÄ Training Logistic Regression [None]...\n",
      "\n",
      "‚úÖ Preprocessing complete! Final features: 32\n",
      "   Training set: 227,845 samples\n",
      "   Test set: 56,962 samples\n",
      "   Label distribution - Train: Normal=0.9983, Fraud=0.0017\n",
      "   Label distribution - Test: Normal=0.9983, Fraud=0.0017\n",
      "‚úÖ Data loaded:\n",
      "   Training: 227,845 samples √ó 32 features\n",
      "   Test: 56,962 samples √ó 32 features\n",
      "   Memory usage: üìä Memory Usage: 2.00 GB\n",
      "   GPU 0 - Allocated: 0.03 GB, Reserved: 0.04 GB\n",
      "   GPU 1 - Allocated: 0.02 GB, Reserved: 0.02 GB\n",
      "\n",
      "================================================================================\n",
      "üî¨ StartinginDataset 'creditCardPCA' running all models\n",
      "================================================================================\n",
      "üìä Â∞Ücomparisonimbalance handlingÊñπÊ≥ï: ['None', 'SMOTE']\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "üìå Imbalance strategy: None\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üöÄ Training Logistic Regression [None]...\n",
      "\n",
      "‚úÖ Logistic Regression completed!\n",
      "   Accuracy: 0.9736\n",
      "   Precision: 0.0562\n",
      "   Recall: 0.9082\n",
      "   F1-Score: 0.1059\n",
      "   ROC-AUC: 0.9733\n",
      "   TrainingTime: 1.13s\n",
      "   Inference time: 0.02s\n",
      "\n",
      "üöÄ Training Random Forest [None]...\n",
      "\n",
      "‚úÖ Logistic Regression completed!\n",
      "   Accuracy: 0.9736\n",
      "   Precision: 0.0562\n",
      "   Recall: 0.9082\n",
      "   F1-Score: 0.1059\n",
      "   ROC-AUC: 0.9733\n",
      "   TrainingTime: 1.13s\n",
      "   Inference time: 0.02s\n",
      "\n",
      "üöÄ Training Random Forest [None]...\n",
      "\n",
      "‚úÖ Random Forest completed!\n",
      "   Accuracy: 0.9993\n",
      "   Precision: 0.7921\n",
      "   Recall: 0.8163\n",
      "   F1-Score: 0.8040\n",
      "   ROC-AUC: 0.9798\n",
      "   TrainingTime: 2.70s\n",
      "   Inference time: 0.09s\n",
      "\n",
      "üöÄ Training XGBoost [None]...\n",
      "\n",
      "‚úÖ Random Forest completed!\n",
      "   Accuracy: 0.9993\n",
      "   Precision: 0.7921\n",
      "   Recall: 0.8163\n",
      "   F1-Score: 0.8040\n",
      "   ROC-AUC: 0.9798\n",
      "   TrainingTime: 2.70s\n",
      "   Inference time: 0.09s\n",
      "\n",
      "üöÄ Training XGBoost [None]...\n",
      "\n",
      "‚úÖ XGBoost completed!\n",
      "   Accuracy: 0.9993\n",
      "   Precision: 0.7757\n",
      "   Recall: 0.8469\n",
      "   F1-Score: 0.8098\n",
      "   ROC-AUC: 0.9734\n",
      "   TrainingTime: 2.22s\n",
      "   Inference time: 0.04s\n",
      "\n",
      "üöÄ Training LightGBM [None]...\n",
      "\n",
      "‚úÖ XGBoost completed!\n",
      "   Accuracy: 0.9993\n",
      "   Precision: 0.7757\n",
      "   Recall: 0.8469\n",
      "   F1-Score: 0.8098\n",
      "   ROC-AUC: 0.9734\n",
      "   TrainingTime: 2.22s\n",
      "   Inference time: 0.04s\n",
      "\n",
      "üöÄ Training LightGBM [None]...\n",
      "\n",
      "‚úÖ LightGBM completed!\n",
      "   Accuracy: 0.9564\n",
      "   Precision: 0.0336\n",
      "   Recall: 0.8776\n",
      "   F1-Score: 0.0648\n",
      "   ROC-AUC: 0.9082\n",
      "   TrainingTime: 3.97s\n",
      "   Inference time: 0.01s\n",
      "\n",
      "üöÄ Training MLP [None]...\n",
      "   Using 2 GPUsfor training\n",
      "\n",
      "‚úÖ LightGBM completed!\n",
      "   Accuracy: 0.9564\n",
      "   Precision: 0.0336\n",
      "   Recall: 0.8776\n",
      "   F1-Score: 0.0648\n",
      "   ROC-AUC: 0.9082\n",
      "   TrainingTime: 3.97s\n",
      "   Inference time: 0.01s\n",
      "\n",
      "üöÄ Training MLP [None]...\n",
      "   Using 2 GPUsfor training\n",
      "   Epoch 10/50 - Train Loss: 0.1127, Val Loss: 0.6603\n",
      "   Epoch 10/50 - Train Loss: 0.1127, Val Loss: 0.6603\n",
      "   Early stopping‰∫éepoch 11\n",
      "\n",
      "‚úÖ MLP completed!\n",
      "   Accuracy: 0.9787\n",
      "   Precision: 0.0688\n",
      "   Recall: 0.9082\n",
      "   F1-Score: 0.1279\n",
      "   ROC-AUC: 0.9798\n",
      "   TrainingTime: 19.43s\n",
      "   Inference time: 0.00s\n",
      "\n",
      "üöÄ Training KNN [None]...\n",
      "   Early stopping‰∫éepoch 11\n",
      "\n",
      "‚úÖ MLP completed!\n",
      "   Accuracy: 0.9787\n",
      "   Precision: 0.0688\n",
      "   Recall: 0.9082\n",
      "   F1-Score: 0.1279\n",
      "   ROC-AUC: 0.9798\n",
      "   TrainingTime: 19.43s\n",
      "   Inference time: 0.00s\n",
      "\n",
      "üöÄ Training KNN [None]...\n",
      "\n",
      "‚úÖ KNN completed!\n",
      "   Accuracy: 0.9996\n",
      "   Precision: 0.9512\n",
      "   Recall: 0.7959\n",
      "   F1-Score: 0.8667\n",
      "   ROC-AUC: 0.9489\n",
      "   TrainingTime: 0.02s\n",
      "   Inference time: 4.69s\n",
      "\n",
      "üöÄ Training PCA + SVM [None]...\n",
      "   PCAdimension reduction: 32 -> 28 dimensions\n",
      "\n",
      "‚úÖ KNN completed!\n",
      "   Accuracy: 0.9996\n",
      "   Precision: 0.9512\n",
      "   Recall: 0.7959\n",
      "   F1-Score: 0.8667\n",
      "   ROC-AUC: 0.9489\n",
      "   TrainingTime: 0.02s\n",
      "   Inference time: 4.69s\n",
      "\n",
      "üöÄ Training PCA + SVM [None]...\n",
      "   PCAdimension reduction: 32 -> 28 dimensions\n"
     ]
    }
   ],
   "source": [
    "# Dataset configuration: optimization strategies for different datasets\n",
    "DATASET_CONFIGS = {\n",
    "    'IEEE': {\n",
    "        'max_samples': None,  # Use FULL dataset (no sampling)\n",
    "        'handle_sparse': True,  # Handle sparse features\n",
    "        'skip_slow': False,  # Run all models\n",
    "        'size_category': 'large',  # 472K samples\n",
    "    },\n",
    "    'col14_behave': {\n",
    "        'max_samples': None,\n",
    "        'handle_sparse': False,\n",
    "        'skip_slow': False,\n",
    "        'size_category': 'medium',  # 238K samples\n",
    "    },\n",
    "    'col16_raw': {\n",
    "        'max_samples': None,\n",
    "        'handle_sparse': False,\n",
    "        'skip_slow': False,\n",
    "        'size_category': 'large',  # 1.47M samples\n",
    "    },\n",
    "    'creditCardPCA': {\n",
    "        'max_samples': None,\n",
    "        'handle_sparse': False,\n",
    "        'skip_slow': False,\n",
    "        'size_category': 'medium',  # 228K samples\n",
    "    },\n",
    "    'creditCardTransaction': {\n",
    "        'max_samples': None,\n",
    "        'handle_sparse': False,\n",
    "        'skip_slow': False,\n",
    "        'size_category': 'large',  # 1.30M samples\n",
    "    },\n",
    "    'counterfeit_products': {\n",
    "        'max_samples': None,\n",
    "        'handle_sparse': False,\n",
    "        'skip_slow': False,\n",
    "        'size_category': 'small',  # 4K samples\n",
    "    },\n",
    "    'counterfeit_transactions': {\n",
    "        'max_samples': None,\n",
    "        'handle_sparse': False,\n",
    "        'skip_slow': False,\n",
    "        'size_category': 'small',  # 2.4K samples\n",
    "    },\n",
    "}\n",
    "\n",
    "# Define datasets grouped by size\n",
    "SMALL_DATASETS = ['counterfeit_products', 'counterfeit_transactions']\n",
    "MEDIUM_DATASETS = ['creditCardPCA', 'col14_behave', 'IEEE']\n",
    "LARGE_DATASETS = ['col16_raw', 'creditCardTransaction']\n",
    "\n",
    "ALL_DATASETS = SMALL_DATASETS + MEDIUM_DATASETS + LARGE_DATASETS\n",
    "\n",
    "# Create experiment runner\n",
    "runner = ExperimentRunner(\n",
    "    compare_imbalance=True,\n",
    "    use_sampling_for_slow_models=False\n",
    ")\n",
    "\n",
    "print(\"üöÄ Starting OPTIMIZED experiment with memory-efficient strategy...\\n\")\n",
    "print(\"=\" * 100)\n",
    "print(\"Experiment Configuration:\")\n",
    "print(f\"  - Compare imbalance handling: Yes\")\n",
    "print(f\"  - GPU acceleration: {'Enabled' if torch.cuda.is_available() else 'Disabled'}\")\n",
    "print(f\"  - Total datasets: {len(ALL_DATASETS)}\")\n",
    "print(f\"  - Small datasets (batch process): {len(SMALL_DATASETS)}\")\n",
    "print(f\"  - Medium datasets (batch process): {len(MEDIUM_DATASETS)}\")\n",
    "print(f\"  - Large datasets (one-by-one): {len(LARGE_DATASETS)}\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "\n",
    "# Strategy 1: Process SMALL & MEDIUM datasets together (low memory overhead)\n",
    "print(f\"\\n{'#'*100}\")\n",
    "print(f\"# PHASE 1: Processing Small & Medium Datasets (Batch Mode)\")\n",
    "print(f\"{'#'*100}\\n\")\n",
    "\n",
    "for dataset_name in SMALL_DATASETS + MEDIUM_DATASETS:\n",
    "    try:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"üìä Loading dataset: {dataset_name}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        config = DATASET_CONFIGS[dataset_name]\n",
    "        \n",
    "        # Load and preprocess data\n",
    "        loader = DatasetLoader(\n",
    "            dataset_name,\n",
    "            max_samples=config['max_samples'],\n",
    "            handle_sparse=config['handle_sparse']\n",
    "        )\n",
    "        train_df, test_df = loader.load_data()\n",
    "        X_train, X_test, y_train, y_test, feature_types = loader.preprocess(\n",
    "            train_df, test_df,\n",
    "            apply_sampling=False\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Data loaded:\")\n",
    "        print(f\"   Training: {X_train.shape[0]:,} samples √ó {X_train.shape[1]} features\")\n",
    "        print(f\"   Test: {X_test.shape[0]:,} samples √ó {X_test.shape[1]} features\")\n",
    "        print(f\"   Memory usage: \", end=\"\")\n",
    "        get_memory_usage()\n",
    "        \n",
    "        # Run all models on this dataset (CORRECT parameter order!)\n",
    "        dataset_results = runner.run_all_models(\n",
    "            X_train, y_train, X_test, y_test,  # ‚úÖ Fixed: X_train, y_train, X_test, y_test\n",
    "            dataset_name=dataset_name,\n",
    "            skip_slow=config['skip_slow']\n",
    "        )\n",
    "        \n",
    "        all_results[dataset_name] = dataset_results\n",
    "        \n",
    "        # IMMEDIATELY clear this dataset from memory\n",
    "        del train_df, test_df, X_train, X_test, y_train, y_test, loader\n",
    "        clear_memory()\n",
    "        \n",
    "        print(f\"\\n‚úÖ {dataset_name} completed and cleared from memory\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {dataset_name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "\n",
    "# Strategy 2: Process LARGE datasets ONE-BY-ONE with aggressive memory management\n",
    "print(f\"\\n{'#'*100}\")\n",
    "print(f\"# PHASE 2: Processing Large Datasets (One-by-One Mode)\")\n",
    "print(f\"{'#'*100}\\n\")\n",
    "\n",
    "for dataset_name in LARGE_DATASETS:\n",
    "    try:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"üìä Loading LARGE dataset: {dataset_name}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        config = DATASET_CONFIGS[dataset_name]\n",
    "        \n",
    "        # Pre-load check\n",
    "        print(f\"‚ö†Ô∏è  Large dataset warning: {dataset_name}\")\n",
    "        print(f\"   This dataset will be loaded and immediately cleared after EACH model\")\n",
    "        print(f\"   to minimize memory usage.\\n\")\n",
    "        \n",
    "        # Load and preprocess data\n",
    "        loader = DatasetLoader(\n",
    "            dataset_name,\n",
    "            max_samples=config['max_samples'],\n",
    "            handle_sparse=config['handle_sparse']\n",
    "        )\n",
    "        train_df, test_df = loader.load_data()\n",
    "        X_train, X_test, y_train, y_test, feature_types = loader.preprocess(\n",
    "            train_df, test_df,\n",
    "            apply_sampling=False\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Data loaded:\")\n",
    "        print(f\"   Training: {X_train.shape[0]:,} samples √ó {X_train.shape[1]} features\")\n",
    "        print(f\"   Test: {X_test.shape[0]:,} samples √ó {X_test.shape[1]} features\")\n",
    "        print(f\"   Memory usage: \", end=\"\")\n",
    "        get_memory_usage()\n",
    "        \n",
    "        # Run all models on this dataset (CORRECT parameter order!)\n",
    "        dataset_results = runner.run_all_models(\n",
    "            X_train, y_train, X_test, y_test,  # ‚úÖ Fixed: X_train, y_train, X_test, y_test\n",
    "            dataset_name=dataset_name,\n",
    "            skip_slow=config['skip_slow']\n",
    "        )\n",
    "        \n",
    "        all_results[dataset_name] = dataset_results\n",
    "        \n",
    "        # AGGRESSIVE memory cleanup for large datasets\n",
    "        del train_df, test_df, X_train, X_test, y_train, y_test, loader\n",
    "        clear_memory()\n",
    "        \n",
    "        # Extra GPU cleanup\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        print(f\"\\n‚úÖ {dataset_name} completed with aggressive memory cleanup\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {dataset_name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        # Cleanup on error\n",
    "        clear_memory()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        continue\n",
    "\n",
    "\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(f\"üéâ ALL EXPERIMENTS COMPLETED!\")\n",
    "print(f\"{'='*100}\\n\")\n",
    "\n",
    "# Save results\n",
    "results_df = pd.DataFrame(all_results).T\n",
    "results_df.to_csv(RESULTS_DIR / 'experiment_results.csv')\n",
    "print(f\"‚úÖ Results saved to: {RESULTS_DIR / 'experiment_results.csv'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5918c1e",
   "metadata": {},
   "source": [
    "## 7. ÁªìÊûúÂàÜÊûêandÂèØËßÜÂåñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3b41c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ëé∑ÂèñÊâÄÊúâÁªìÊûú\n",
    "results_df = runner.evaluator.get_results_df()\n",
    "\n",
    "# ÂàõÂª∫ÂàÜÊûêÂô®\n",
    "analyzer = ResultsAnalyzer(results_df)\n",
    "\n",
    "# Generate summary table\n",
    "analyzer.generate_summary_table()\n",
    "\n",
    "# ‰øùÂ≠òÁªìÊûú\n",
    "analyzer.export_results(RESULTS_DIR / 'experiment_results.csv')\n",
    "print(f\"\\nÂÆåÊï¥ÁªìÊûúÂ∑≤‰øùÂ≠ò!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd626150",
   "metadata": {},
   "source": [
    "### 7.0 imbalance handlingÊïàÊûúcomparison\n",
    "\n",
    "È¶ñÂÖàÂàÜÊûê‰∏çÂêåimbalance handlingÊñπÊ≥ïofÊïàÊûú„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005a1227",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.plot_imbalance_comparison('f1_score', figsize=(18, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97b79a5",
   "metadata": {},
   "source": [
    "### 7.1 F1-Score comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6deb289",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.plot_metric_comparison('f1_score', figsize=(16, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b139038e",
   "metadata": {},
   "source": [
    "### 7.2 ROC-AUC comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8520fd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.plot_metric_comparison('roc_auc', figsize=(16, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769d65f5",
   "metadata": {},
   "source": [
    "### 7.3 ÊâÄÊúâÊåáÊ†áÁÉ≠ÂäõÂõæ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31236248",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.plot_all_metrics_heatmap(figsize=(18, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a10e69",
   "metadata": {},
   "source": [
    "### 7.4 TimeÊÄßËÉΩcomparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2ecdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.plot_time_comparison(figsize=(16, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293d69b0",
   "metadata": {},
   "source": [
    "### 7.5 DatasetdimensionsÂ∫¶comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f8a9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.plot_dataset_comparison(figsize=(16, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e0da2e",
   "metadata": {},
   "source": [
    "## 8. Ê∑±ÂÖ•ÂàÜÊûêÔºöÂçïDatasetÁ§∫‰æã\n",
    "\n",
    "‰ª•‰∏ãÊºîÁ§∫Â¶Ç‰ΩïinÂçïDataset‰∏äËøõË°åËØ¶ÁªÜÂàÜÊûêÔºàÂèØÈÄâÊã©‰ªªÊÑèDatasetÔºâ„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b356a323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÈÄâÊã©‰∏ÄDatasetËøõË°åËØ¶ÁªÜÂàÜÊûê\n",
    "selected_dataset = 'creditCardPCA'\n",
    "\n",
    "# ÈáçÊñ∞Âä†ËΩΩÊï∞ÊçÆ\n",
    "loader = DatasetLoader(selected_dataset)\n",
    "train_df, test_df = loader.load_data()\n",
    "X_train, X_test, y_train, y_test, feature_types = loader.preprocess(train_df, test_df)\n",
    "\n",
    "print(f\"\\nüìä Dataset: {selected_dataset}\")\n",
    "print(f\"TrainingÈõÜÂ§ßÂ∞è: {X_train.shape}\")\n",
    "print(f\"Test setÂ§ßÂ∞è: {X_test.shape}\")\n",
    "print(f\"featuresÊï∞Èáè: {X_train.shape[1]}\")\n",
    "print(f\"Á±ªÂà´ÂàÜÂ∏É (TrainingÈõÜ): {dict(y_train.value_counts())}\")\n",
    "print(f\"Á±ªÂà´ÂàÜÂ∏É (Test set): {dict(y_test.value_counts())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c064435d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‰ºòÂåñÔºöËá™Âä®ËØªÂèñÊúÄÊñ∞samplesÈáèÔºåÊîØÊåÅÂàÜÂ±ÇÈááÊ†∑andÂ§öÊ¨°ÈáçÂ§çÂÆûÈ™å\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "selected_dataset = 'creditCardPCA'  # ÂèØÊõ¥Êîπas‰ªªÊÑèDataset\n",
    "n_repeats = 5  # ÈáçÂ§çÂÆûÈ™åÊ¨°Êï∞Ôºå‰øùËØÅÁªüËÆ°Â≠¶‰∏•Ë∞®\n",
    "split_ratio = 0.2  # Test setratio\n",
    "random_seed = 42\n",
    "\n",
    "# ÈáçÊñ∞Âä†ËΩΩÊï∞ÊçÆ\n",
    "loader = DatasetLoader(selected_dataset)\n",
    "train_df, test_df = loader.load_data()\n",
    "\n",
    "# ÂêàÂπ∂ÂÖ®ÈáèÊï∞ÊçÆÔºàÂ¶ÇÊúâÈúÄË¶ÅÔºâ\n",
    "full_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "X = full_df.drop(columns=['is_fraud'])\n",
    "y = full_df['is_fraud']\n",
    "\n",
    "print(f\"\\nüìä Dataset: {selected_dataset}\")\n",
    "print(f\"ÂÖ®Èáèsamples: {len(full_df)}ÔºåÁ±ªÂà´ÂàÜÂ∏É: {dict(y.value_counts())}\")\n",
    "\n",
    "# Â§öÊ¨°ÂàÜÂ±ÇÈááÊ†∑ÂàíÂàÜTraining/Test set\n",
    "for repeat in range(n_repeats):\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=split_ratio, random_state=random_seed + repeat)\n",
    "    for train_idx, test_idx in sss.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    print(f\"\\nÁ¨¨{repeat+1}Ê¨°ÂàíÂàÜÔºö\")\n",
    "    print(f\"TrainingÈõÜ: {X_train.shape}, Test set: {X_test.shape}\")\n",
    "    print(f\"TrainingÈõÜÁ±ªÂà´ÂàÜÂ∏É: {dict(y_train.value_counts())}\")\n",
    "    print(f\"Test setÁ±ªÂà´ÂàÜÂ∏É: {dict(y_test.value_counts())}\")\n",
    "    # ËøôÈáåÂèØÊèíÂÖ•ModelTrainingandËØÑ‰º∞‰ª£Á†ÅÔºåÊî∂ÈõÜÊØèÊ¨°ÂÆûÈ™åÁªìÊûú\n",
    "\n",
    "# ÂêéÁª≠ÂèØfor n_repeats Ê¨°ÂÆûÈ™åÁªìÊûúÂÅöÂùáValue/ÊñπÂ∑ÆÁªüËÆ°andÊòæËëóÊÄßÊ£ÄÈ™å\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52704cad",
   "metadata": {},
   "source": [
    "### 8.1 Êï∞ÊçÆÂàÜÂ∏ÉÂèØËßÜÂåñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba731050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÁªòÂà∂Label distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# TrainingÈõÜLabel distribution\n",
    "y_train.value_counts().plot(kind='bar', ax=axes[0], color=['skyblue', 'coral'])\n",
    "axes[0].set_title('TrainingÈõÜLabel distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Ê†áÁ≠æ (0=Normal, 1=Fraud)', fontsize=12)\n",
    "axes[0].set_ylabel('samplesÊï∞Èáè', fontsize=12)\n",
    "axes[0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Test setLabel distribution\n",
    "y_test.value_counts().plot(kind='bar', ax=axes[1], color=['skyblue', 'coral'])\n",
    "axes[1].set_title('Test setLabel distribution', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Ê†áÁ≠æ (0=Normal, 1=Fraud)', fontsize=12)\n",
    "axes[1].set_ylabel('samplesÊï∞Èáè', fontsize=12)\n",
    "axes[1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# featuresÁõ∏ÂÖ≥ÊÄßÁÉ≠ÂäõÂõæÔºàÂè™ÊòæÁ§∫Ââç20featuresÔºâ\n",
    "if X_train.shape[1] <= 20:\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    corr = X_train.corr()\n",
    "    sns.heatmap(corr, annot=False, cmap='coolwarm', center=0)\n",
    "    plt.title(f'{selected_dataset} - featuresÁõ∏ÂÖ≥ÊÄßÁü©Èòµ', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"featuresÊï∞ÈáèËæÉÂ§ö ({X_train.shape[1]}), Ë∑≥ËøáÁõ∏ÂÖ≥ÊÄßÁÉ≠ÂäõÂõæ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b84ad59",
   "metadata": {},
   "source": [
    "## 9. ÊÄªÁªìandÂª∫ËÆÆ\n",
    "\n",
    "Ê†πÊçÆÂÆûÈ™åÁªìÊûúÔºåÊàë‰ª¨ÂèØ‰ª•ÂæóÂá∫‰ª•‰∏ãÁªìËÆ∫andÂª∫ËÆÆ„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fb990e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÁîüÊàêËØ¶ÁªÜofÂàÜÊûêÊä•Âëä\n",
    "print(\"=\"*100)\n",
    "print(\"üìù ÂÆûÈ™åÊÄªÁªìandÂàÜÊûê\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# 1. BestModelÂàÜÊûê\n",
    "print(\"\\n### 1Ô∏è‚É£ BestModelÂàÜÊûê\\n\")\n",
    "best_models = {}\n",
    "for metric in ['f1_score', 'roc_auc', 'precision', 'recall']:\n",
    "    best_idx = results_df[metric].idxmax()\n",
    "    best = results_df.loc[best_idx]\n",
    "    best_models[metric] = best\n",
    "    print(f\"üèÜ Best {metric.upper()}: {best['model']} (Dataset: {best['dataset']}, Value: {best[metric]:.4f})\")\n",
    "\n",
    "# 2. ModelÁ±ªÂà´ÂàÜÊûê\n",
    "print(\"\\n### 2Ô∏è‚É£ ModelÁ±ªÂà´ÊÄßËÉΩÂàÜÊûê\\n\")\n",
    "\n",
    "# ÁõëÁù£Â≠¶‰π†ÊñπÊ≥ï\n",
    "supervised = ['Logistic Regression', 'Random Forest', 'XGBoost', 'LightGBM', 'MLP', 'KNN']\n",
    "supervised_results = results_df[results_df['model'].isin(supervised)]\n",
    "print(f\"üìä ÁõëÁù£Â≠¶‰π†ÊñπÊ≥ïAverage F1-Score: {supervised_results['f1_score'].mean():.4f}\")\n",
    "\n",
    "# dimension reduction+ÂàÜÁ±ªÊñπÊ≥ï\n",
    "dim_reduction = ['PCA+SVM', 'PCA+LR']\n",
    "dim_reduction_results = results_df[results_df['model'].isin(dim_reduction)]\n",
    "if not dim_reduction_results.empty:\n",
    "    print(f\"üìä dimension reduction+ÂàÜÁ±ªÊñπÊ≥ïAverage F1-Score: {dim_reduction_results['f1_score'].mean():.4f}\")\n",
    "\n",
    "# Êó†ÁõëÁù£ÊñπÊ≥ï\n",
    "unsupervised = ['Isolation Forest', 'One-Class SVM', 'Autoencoder']\n",
    "unsupervised_results = results_df[results_df['model'].isin(unsupervised)]\n",
    "if not unsupervised_results.empty:\n",
    "    print(f\"üìä Êó†ÁõëÁù£ÊñπÊ≥ïAverage F1-Score: {unsupervised_results['f1_score'].mean():.4f}\")\n",
    "\n",
    "# 3. DatasetÁâπÊÄßÂàÜÊûê\n",
    "print(\"\\n### 3Ô∏è‚É£ DatasetÁâπÊÄßforModelÊÄßËÉΩofÂΩ±Âìç\\n\")\n",
    "for dataset in results_df['dataset'].unique():\n",
    "    dataset_results = results_df[results_df['dataset'] == dataset]\n",
    "    best_model = dataset_results.loc[dataset_results['f1_score'].idxmax()]\n",
    "    avg_f1 = dataset_results['f1_score'].mean()\n",
    "    print(f\"üìÅ {dataset:25s} - Best: {best_model['model']:20s} (F1: {best_model['f1_score']:.4f}), \"\n",
    "          f\"Average: {avg_f1:.4f}\")\n",
    "\n",
    "# 4. ÊïàÁéáÂàÜÊûê\n",
    "print(\"\\n### 4Ô∏è‚É£ TrainingandÊé®ÁêÜÊïàÁéáÂàÜÊûê\\n\")\n",
    "time_analysis = results_df.groupby('model')[['train_time', 'inference_time']].mean()\n",
    "time_analysis = time_analysis.sort_values('train_time')\n",
    "print(\"‚è±Ô∏è  TrainingTimeÊéíÂêçÔºàÂø´‚ÜíÊÖ¢Ôºâ:\")\n",
    "for idx, (model, row) in enumerate(time_analysis.iterrows(), 1):\n",
    "    print(f\"   {idx}. {model:20s} - Training: {row['train_time']:6.2f}s, Êé®ÁêÜ: {row['inference_time']:.4f}s\")\n",
    "\n",
    "# 5. ÁªºÂêàÊé®Ëçê\n",
    "print(\"\\n### 5Ô∏è‚É£ ModelÊé®ËçêÂª∫ËÆÆ\\n\")\n",
    "print(\"Âü∫‰∫éÂÆûÈ™åÁªìÊûúÔºåÈíàfor‰∏çÂêåÂú∫ÊôØofÊé®ËçêÔºö\")\n",
    "print(\"\\nüéØ **È´òAccuracyÂú∫ÊôØ**ÔºàËøΩÊ±ÇBestÊÄßËÉΩÔºâ:\")\n",
    "best_f1_model = results_df.groupby('model')['f1_score'].mean().idxmax()\n",
    "print(f\"   Êé®Ëçê: {best_f1_model}\")\n",
    "\n",
    "print(\"\\n‚ö° **ÂÆûÊó∂Êé®ÁêÜÂú∫ÊôØ**ÔºàÈÄüÂ∫¶‰ºòÂÖàÔºâ:\")\n",
    "fast_models = results_df.groupby('model')['inference_time'].mean().nsmallest(3)\n",
    "print(f\"   Êé®Ëçê: {', '.join(fast_models.index.tolist())}\")\n",
    "\n",
    "print(\"\\nüí∞ **ËµÑÊ∫êÂèóÈôêÂú∫ÊôØ**Ôºà‰ΩéËÆ°ÁÆóÊàêÊú¨Ôºâ:\")\n",
    "efficient_models = results_df.groupby('model')['train_time'].mean().nsmallest(3)\n",
    "print(f\"   Êé®Ëçê: {', '.join(efficient_models.index.tolist())}\")\n",
    "\n",
    "print(\"\\nüîç **class imbalanceÂú∫ÊôØ**ÔºàFraudÊ£ÄÊµãÁâπÊÄßÔºâ:\")\n",
    "best_recall_model = results_df.groupby('model')['recall'].mean().idxmax()\n",
    "print(f\"   Êé®Ëçê: {best_recall_model} (È´òRecall)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22880d3",
   "metadata": {},
   "source": [
    "## 10. DatasetÁâπÊÄßÊ¶ÇËßà"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13feb34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ËØªÂèñDatasetÂÖÉ‰ø°ÊÅØ\n",
    "with open(JSON_DIR / 'dataset_clean_summary.json', 'r') as f:\n",
    "    dataset_info = json.load(f)\n",
    "\n",
    "# ÊèêÂèñÂÖ≥ÈîÆ‰ø°ÊÅØ\n",
    "dataset_summary = []\n",
    "for dataset_name, info in dataset_info.items():\n",
    "    if 'train' in info and info['train']:\n",
    "        train_info = info['train'][0]\n",
    "        test_info = info['test'][0] if 'test' in info and info['test'] else train_info\n",
    "        \n",
    "        # Ëé∑ÂèñFraud Rate\n",
    "        fraud_rate = 0\n",
    "        if 'label_distribution' in train_info:\n",
    "            label_dist = train_info['label_distribution']\n",
    "            if 'is_fraud' in label_dist:\n",
    "                fraud_rate = label_dist['is_fraud'].get('1', 0)\n",
    "            elif 'isFraud' in label_dist:\n",
    "                fraud_rate = label_dist['isFraud'].get('1', 0)\n",
    "        \n",
    "        dataset_summary.append({\n",
    "            'Dataset': dataset_name,\n",
    "            'Train Samples': train_info['n_rows'],\n",
    "            'Test Samples': test_info['n_rows'],\n",
    "            'Features': train_info['n_cols'] - 1,  # ÂáèÂéªÊ†áÁ≠æÂàó\n",
    "            'Fraud Rate (%)': fraud_rate * 100\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(dataset_summary)\n",
    "summary_df = summary_df.sort_values('Train Samples', ascending=False)\n",
    "\n",
    "# ÊòæÁ§∫Ë°®Ê†º\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üìä DatasetÁâπÊÄßÊÄªËßà\")\n",
    "print(\"=\"*100 + \"\\n\")\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"\\n\")\n",
    "\n",
    "# ÂèØËßÜÂåñDatasetÁâπÊÄß\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# 1. samplesÈáècomparison\n",
    "ax = axes[0, 0]\n",
    "x = np.arange(len(summary_df))\n",
    "width = 0.35\n",
    "ax.bar(x - width/2, summary_df['Train Samples'], width, label='TrainingÈõÜ', alpha=0.8)\n",
    "ax.bar(x + width/2, summary_df['Test Samples'], width, label='Test set', alpha=0.8)\n",
    "ax.set_xlabel('Dataset', fontsize=12)\n",
    "ax.set_ylabel('samplesÊï∞Èáè', fontsize=12)\n",
    "ax.set_title('DatasetsamplesÈáècomparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(summary_df['Dataset'], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 2. featuresÊï∞Èáècomparison\n",
    "ax = axes[0, 1]\n",
    "ax.barh(summary_df['Dataset'], summary_df['Features'], color='skyblue')\n",
    "ax.set_xlabel('featuresÊï∞Èáè', fontsize=12)\n",
    "ax.set_ylabel('Dataset', fontsize=12)\n",
    "ax.set_title('featuresÊï∞Èáècomparison', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 3. Fraud Ratecomparison\n",
    "ax = axes[1, 0]\n",
    "colors = ['green' if x < 5 else 'orange' if x < 20 else 'red' \n",
    "          for x in summary_df['Fraud Rate (%)']]\n",
    "ax.barh(summary_df['Dataset'], summary_df['Fraud Rate (%)'], color=colors, alpha=0.7)\n",
    "ax.set_xlabel('Fraud Rate (%)', fontsize=12)\n",
    "ax.set_ylabel('Dataset', fontsize=12)\n",
    "ax.set_title('class imbalanceÁ®ãÂ∫¶ (Fraud Rate)', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "# Ê∑ªÂä†ÈòàValueÁ∫ø\n",
    "ax.axvline(x=5, color='orange', linestyle='--', alpha=0.5, label='inÂ∫¶imbalance')\n",
    "ax.axvline(x=20, color='red', linestyle='--', alpha=0.5, label='ËΩªÂ∫¶imbalance')\n",
    "ax.legend()\n",
    "\n",
    "# 4. DatasetËßÑÊ®°Êï£ÁÇπÂõæ\n",
    "ax = axes[1, 1]\n",
    "scatter = ax.scatter(summary_df['Features'], summary_df['Train Samples'], \n",
    "                    s=summary_df['Fraud Rate (%)'] * 100, \n",
    "                    c=summary_df['Fraud Rate (%)'], \n",
    "                    cmap='RdYlGn_r', alpha=0.6, edgecolors='black')\n",
    "for idx, row in summary_df.iterrows():\n",
    "    ax.annotate(row['Dataset'], \n",
    "               (row['Features'], row['Train Samples']),\n",
    "               fontsize=9, ha='center')\n",
    "ax.set_xlabel('featuresÊï∞Èáè', fontsize=12)\n",
    "ax.set_ylabel('TrainingsamplesÊï∞Èáè', fontsize=12)\n",
    "ax.set_title('DatasetËßÑÊ®°ÂàÜÂ∏É (Ê∞îÊ≥°Â§ßÂ∞è=Fraud Rate)', fontsize=14, fontweight='bold')\n",
    "ax.grid(alpha=0.3)\n",
    "plt.colorbar(scatter, ax=ax, label='Fraud Rate (%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° DatasetÁâπÊÄßÂàÜÊûê:\")\n",
    "print(\"  - ÊúÄÂ§ßDataset:\", summary_df.iloc[0]['Dataset'], \n",
    "      f\"({summary_df.iloc[0]['Train Samples']:,} samples)\")\n",
    "print(\"  - ÊúÄÂ∞èDataset:\", summary_df.iloc[-1]['Dataset'], \n",
    "      f\"({summary_df.iloc[-1]['Train Samples']:,} samples)\")\n",
    "print(\"  - ÊúÄÈ´òdimensionsÂ∫¶:\", summary_df.loc[summary_df['Features'].idxmax()]['Dataset'],\n",
    "      f\"({summary_df['Features'].max()} features)\")\n",
    "print(\"  - ÊúÄimbalance:\", summary_df.loc[summary_df['Fraud Rate (%)'].idxmin()]['Dataset'],\n",
    "      f\"({summary_df['Fraud Rate (%)'].min():.2f}% Fraud Rate)\")\n",
    "print(\"  - ÊúÄÂπ≥Ë°°:\", summary_df.loc[summary_df['Fraud Rate (%)'].idxmax()]['Dataset'],\n",
    "      f\"({summary_df['Fraud Rate (%)'].max():.2f}% Fraud Rate)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47512213",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù ÂÆûÈ™åcompletedÊ∏ÖÂçï\n",
    "\n",
    "completedÂÆûÈ™åÂêéÔºå‰Ω†Â∫îËØ•ÂæótoÔºö\n",
    "\n",
    "‚úÖ **Êï∞ÊçÆÂàÜÊûê**\n",
    "- [ ] 7DatasetofËØ¶ÁªÜÊèèËø∞\n",
    "- [ ] Êï∞ÊçÆÂàÜÂ∏ÉÂèØËßÜÂåñ\n",
    "- [ ] class imbalanceÂàÜÊûê\n",
    "\n",
    "‚úÖ **ModelTraining**\n",
    "- [ ] Ëá≥Â∞ë8ModelinÊØèDataset‰∏äofTraining\n",
    "- [ ] ÂÆåÊï¥ofTrainingÊó•Âøó\n",
    "- [ ] Model‰øùÂ≠òÔºàÂèØÈÄâÔºâ\n",
    "\n",
    "‚úÖ **ËØÑ‰º∞ÁªìÊûú**\n",
    "- [ ] `experiment_results.csv` Êñá‰ª∂\n",
    "- [ ] ÊâÄÊúâ# Evaluation metricsÔºàAccuracy, Precision, Recall, F1, AUCÔºâ\n",
    "- [ ] TimeÊÄßËÉΩËÆ∞ÂΩï\n",
    "\n",
    "‚úÖ **ÂèØËßÜÂåñÂàÜÊûê**\n",
    "- [ ] F1-ScorecomparisonÂõæ\n",
    "- [ ] ROC-AUCcomparisonÂõæ\n",
    "- [ ] ÊåáÊ†áÁÉ≠ÂäõÂõæ\n",
    "- [ ] TimeÊÄßËÉΩÂõæ\n",
    "- [ ] DatasetdimensionsÂ∫¶comparison\n",
    "\n",
    "‚úÖ **ÂàÜÊûêÊä•Âëä**\n",
    "- [ ] BestModelÊÄªÁªì\n",
    "- [ ] ModelÁ±ªÂà´ÊÄßËÉΩÂàÜÊûê\n",
    "- [ ] DatasetÁâπÊÄßÂΩ±ÂìçÂàÜÊûê\n",
    "- [ ] ÊïàÁéáÂàÜÊûê\n",
    "- [ ] UsingÂª∫ËÆÆ\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ ‰∏ã‰∏ÄÊ≠•Âª∫ËÆÆ\n",
    "\n",
    "### ËøõÈò∂ÂÆûÈ™å\n",
    "1. **featuresÂ∑•Á®ã**: Â∞ùËØïÂàõÂª∫Êñ∞featuresImprovementModelÊÄßËÉΩ\n",
    "2. **Ë∂ÖÂèÇÊï∞Ë∞É‰ºò**: UsingGridSearchorBayesian‰ºòÂåñ\n",
    "3. **ÈõÜÊàêÂ≠¶‰π†**: ÁªÑÂêàÂ§öModelofÈ¢ÑÊµãÁªìÊûú\n",
    "4. **# Deep learning**: Â∞ùËØïLSTM„ÄÅTransformeretcÊû∂ÊûÑ\n",
    "5. **Ëß£ÈáäÊÄßÂàÜÊûê**: UsingSHAPValueÂàÜÊûêfeaturesÈáçË¶ÅÊÄß\n",
    "\n",
    "### ËÆ∫ÊñáÊí∞ÂÜô\n",
    "1. **ÊñπÊ≥ïËÆ∫**: ËØ¶ÁªÜÊèèËø∞# Data preprocessingandModelÈÄâÊã©ÁêÜÁî±\n",
    "2. **ÂÆûÈ™å# Settings**: ËÆ∞ÂΩïÊâÄÊúâË∂ÖÂèÇÊï∞andÁ°¨‰ª∂Configuration\n",
    "3. **ÁªìÊûúÂàÜÊûê**: Ê∑±ÂÖ•ÂàÜÊûêas‰ªÄ‰πàÊüê‰∫õModelË°®Áé∞Êõ¥Â•Ω\n",
    "4. **ËÆ®ËÆ∫**: ÊØîËæÉ‰Ω†ofÁªìÊûúandÊñáÁåÆinofÁªìÊûú\n",
    "5. **ÁªìËÆ∫**: ÊÄªÁªìÂÖ≥ÈîÆÂèëÁé∞andÂÆûË∑µÂª∫ËÆÆ\n",
    "\n",
    "### ‰ª£Á†Å‰ºòÂåñ\n",
    "1. **Âπ∂Ë°åTraining**: UsingjoblibÂπ∂Ë°åTrainingÂ§öModel\n",
    "2. **Â¢ûÈáèÂ≠¶‰π†**: for‰∫éÂ§ßDatasetUsingmini-batchTraining\n",
    "3. **ModelÊåÅ‰πÖÂåñ**: ‰øùÂ≠òTrainingÂ•ΩofModel‰ª•‰æøÂ§çwith\n",
    "4. **Êó•ÂøóËÆ∞ÂΩï**: UsingloggingÊ®°ÂùóËÆ∞ÂΩïËØ¶ÁªÜÊó•Âøó\n",
    "5. **ConfigurationÁÆ°ÁêÜ**: UsingYAMLConfigurationÊñá‰ª∂ÁÆ°ÁêÜÂèÇÊï∞\n",
    "\n",
    "Á•ù‰Ω†ÂÆûÈ™åÈ°∫Âà©ÔºÅüöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FYP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
